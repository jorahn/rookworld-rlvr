[
  {
    "epoch": 0,
    "timing": {
      "total_time": 0.14484402800007956,
      "forward_time": 0.011453120001533534,
      "logprob_time": 0.0042252269995515235,
      "loss_time": 0.030707414000062272,
      "backward_time": 0.056318823000765406,
      "optim_time": 0.0421368489987799
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 6519086920420.179,
      "mfu_percent": 3.9509617699516233
    },
    "training_dynamics": {
      "loss": 0.10157471895217896,
      "current_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        0.0,
        0.0
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.12332374602556229,
      "kl_loss": -0.02174903079867363,
      "kl_div": -2.174903154373169,
      "kl_coef": 0.01,
      "total_loss": 0.10157471895217896,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.12688866257667542,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 9.911557728163801,
      "clipped_grad_norm": 9.9115571975708,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.045810479670763016
    },
    "memory": {
      "allocated_gb": 2.725900173187256,
      "reserved_gb": 3.375,
      "peak_gb": 3.1902194023132324
    }
  },
  {
    "epoch": 1,
    "timing": {
      "total_time": 0.04294674100674456,
      "forward_time": 0.0055230500001925975,
      "logprob_time": 0.004005557006166782,
      "loss_time": 0.010115763994690496,
      "backward_time": 0.00926673900539754,
      "optim_time": 0.014034690000698902
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 13518596572074.55,
      "mfu_percent": 8.193088831560335
    },
    "training_dynamics": {
      "loss": 0.15143197774887085,
      "current_logprobs": [
        -0.21742531657218933,
        -1.7334635257720947
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -0.004877716302871704,
        0.028250575065612793
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.16040688753128052,
      "kl_loss": -0.008974906988441944,
      "kl_div": -0.8974906802177429,
      "kl_coef": 0.01,
      "total_loss": 0.15143197774887085,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.5717912912368774,
      "fraction_clipped": 0.5
    },
    "gradients": {
      "total_grad_norm": 81.70546805298041,
      "clipped_grad_norm": 81.7054672241211,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.03245855122804642
    },
    "memory": {
      "allocated_gb": 2.7521910667419434,
      "reserved_gb": 3.337890625,
      "peak_gb": 3.21651029586792
    }
  },
  {
    "epoch": 2,
    "timing": {
      "total_time": 0.0416664820004371,
      "forward_time": 0.004447232000529766,
      "logprob_time": 0.003949743004341144,
      "loss_time": 0.009877238000626676,
      "backward_time": 0.009346426995762158,
      "optim_time": 0.014045390002138447
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16788844114969.904,
      "mfu_percent": 10.1750570393757
    },
    "training_dynamics": {
      "loss": 0.0917172059416771,
      "current_logprobs": [
        -0.23329700529575348,
        -1.811108112335205
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -0.020749405026435852,
        -0.04939401149749756
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.15998342633247375,
      "kl_loss": -0.06826622039079666,
      "kl_div": -6.826622009277344,
      "kl_coef": 0.01,
      "total_loss": 0.0917172059416771,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.00713352020829916,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 6.696175454583449,
      "clipped_grad_norm": 6.6961750984191895,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.025825630873441696
    },
    "memory": {
      "allocated_gb": 2.752480983734131,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.2168002128601074
    }
  },
  {
    "epoch": 3,
    "timing": {
      "total_time": 0.04291739600012079,
      "forward_time": 0.005422403002739884,
      "logprob_time": 0.004237961002218071,
      "loss_time": 0.009954222005035263,
      "backward_time": 0.009313665999798104,
      "optim_time": 0.013988292994326912
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 13769519669097.467,
      "mfu_percent": 8.345163435816646
    },
    "training_dynamics": {
      "loss": 0.10453726351261139,
      "current_logprobs": [
        -0.27524638175964355,
        -1.9020962715148926
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -0.06269878149032593,
        -0.14038217067718506
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.15924572944641113,
      "kl_loss": -0.054708462208509445,
      "kl_div": -5.470846176147461,
      "kl_coef": 0.01,
      "total_loss": 0.10453726351261139,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0042329044081270695,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.279456009143019,
      "clipped_grad_norm": 2.27945613861084,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.028462903574109077
    },
    "memory": {
      "allocated_gb": 2.7522521018981934,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.21730375289917
    }
  },
  {
    "epoch": 4,
    "timing": {
      "total_time": 0.04117842100095004,
      "forward_time": 0.0045886750012869015,
      "logprob_time": 0.0039659030007896945,
      "loss_time": 0.009790937998332083,
      "backward_time": 0.008868634999089409,
      "optim_time": 0.013963726996735204
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16271338628048.486,
      "mfu_percent": 9.861417350332417
    },
    "training_dynamics": {
      "loss": 0.014496639370918274,
      "current_logprobs": [
        -0.31838515400886536,
        -2.0458526611328125
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -0.10583755373954773,
        -0.284138560295105
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.15999986231327057,
      "kl_loss": -0.1455032229423523,
      "kl_div": -14.550322532653809,
      "kl_coef": 0.01,
      "total_loss": 0.014496639370918274,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 5.322402785168379e-07,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 3.066840831288701,
      "clipped_grad_norm": 3.066840648651123,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.024694588035345078
    },
    "memory": {
      "allocated_gb": 2.751809597015381,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.2168612480163574
    }
  },
  {
    "epoch": 5,
    "timing": {
      "total_time": 0.040868481002689805,
      "forward_time": 0.0043864479957846925,
      "logprob_time": 0.00395560500328429,
      "loss_time": 0.00964144800673239,
      "backward_time": 0.008926604001317173,
      "optim_time": 0.013957896000647452
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 17021490935661.568,
      "mfu_percent": 10.316055112522163
    },
    "training_dynamics": {
      "loss": 0.020055368542671204,
      "current_logprobs": [
        -0.3428810238838196,
        -2.3261983394622803
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -0.13033342361450195,
        -0.5644842386245728
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.1399446427822113,
      "kl_div": -13.994464874267578,
      "kl_coef": 0.01,
      "total_loss": 0.020055368542671204,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 6.712261529173702e-05,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.1045067010200236,
      "clipped_grad_norm": 2.104506731033325,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.02315143682062626
    },
    "memory": {
      "allocated_gb": 2.7511839866638184,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.216723918914795
    }
  },
  {
    "epoch": 6,
    "timing": {
      "total_time": 0.04097362700122176,
      "forward_time": 0.004447001003427431,
      "logprob_time": 0.0039901490017655306,
      "loss_time": 0.009785677000763826,
      "backward_time": 0.00878722299967194,
      "optim_time": 0.013963106001028791
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16789716202549.629,
      "mfu_percent": 10.175585577302805
    },
    "training_dynamics": {
      "loss": -0.04268735647201538,
      "current_logprobs": [
        -0.36379507184028625,
        -2.882086992263794
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -0.15124747157096863,
        -1.1203728914260864
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.2026873677968979,
      "kl_div": -20.26873779296875,
      "kl_coef": 0.01,
      "total_loss": -0.04268735647201538,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 7.479678210131624e-09,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.949653326758646,
      "clipped_grad_norm": 1.9496532678604126,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.020590657368302345
    },
    "memory": {
      "allocated_gb": 2.752999782562256,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.2189974784851074
    }
  },
  {
    "epoch": 7,
    "timing": {
      "total_time": 0.04064468399883481,
      "forward_time": 0.004423528000188526,
      "logprob_time": 0.003938722999009769,
      "loss_time": 0.00955445599538507,
      "backward_time": 0.008765062004385982,
      "optim_time": 0.013962454999273177
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16878809130815.473,
      "mfu_percent": 10.229581291403317
    },
    "training_dynamics": {
      "loss": -0.05609768629074097,
      "current_logprobs": [
        -0.35821062326431274,
        -4.350047588348389
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -0.14566302299499512,
        -2.5883336067199707
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.21609769761562347,
      "kl_div": -21.609769821166992,
      "kl_coef": 0.01,
      "total_loss": -0.05609768629074097,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 4.3041720187986243e-10,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.5210682765901058,
      "clipped_grad_norm": 1.5210683345794678,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.018800804391503334
    },
    "memory": {
      "allocated_gb": 2.7525572776794434,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.21809720993042
    }
  },
  {
    "epoch": 8,
    "timing": {
      "total_time": 0.04094147800060455,
      "forward_time": 0.004467049002414569,
      "logprob_time": 0.0039235849981196225,
      "loss_time": 0.009629715998016763,
      "backward_time": 0.008954256001743488,
      "optim_time": 0.013966402002552059
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16714364395743.59,
      "mfu_percent": 10.129917815602175
    },
    "training_dynamics": {
      "loss": -0.07640761137008667,
      "current_logprobs": [
        -0.4912002682685852,
        -8.988856315612793
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -0.2786526679992676,
        -7.227142333984375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.23640762269496918,
      "kl_div": -23.640762329101562,
      "kl_coef": 0.01,
      "total_loss": -0.07640761137008667,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 5.821357285107354e-11,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 3.5614771087751635,
      "clipped_grad_norm": 3.5614771842956543,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0179890263825655
    },
    "memory": {
      "allocated_gb": 2.7525572776794434,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.21809720993042
    }
  },
  {
    "epoch": 9,
    "timing": {
      "total_time": 0.040906021000409964,
      "forward_time": 0.004451348999282345,
      "logprob_time": 0.003933522995794192,
      "loss_time": 0.009736756997881457,
      "backward_time": 0.008833609994326252,
      "optim_time": 0.013950332002423238
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16773316316477.871,
      "mfu_percent": 10.16564625241083
    },
    "training_dynamics": {
      "loss": -0.09338392317295074,
      "current_logprobs": [
        -1.692923665046692,
        -14.79217529296875
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -1.4803760051727295,
        -13.030461311340332
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.25338393449783325,
      "kl_div": -25.338394165039062,
      "kl_coef": 0.01,
      "total_loss": -0.09338392317295074,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.238493942817831e-11,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.3204727801164902,
      "clipped_grad_norm": 1.3204727172851562,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.019006671383976936
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 10,
    "timing": {
      "total_time": 0.04068660199845908,
      "forward_time": 0.004445828999450896,
      "logprob_time": 0.003910330000508111,
      "loss_time": 0.00962907500070287,
      "backward_time": 0.008750034001423046,
      "optim_time": 0.013950452004792169
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16794142286898.963,
      "mfu_percent": 10.178268052666038
    },
    "training_dynamics": {
      "loss": -0.0989006906747818,
      "current_logprobs": [
        -7.712220191955566,
        -21.495010375976562
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -7.4996724128723145,
        -19.733295440673828
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.2589007019996643,
      "kl_div": -25.890071868896484,
      "kl_coef": 0.01,
      "total_loss": -0.0989006906747818,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 6.093784331306917e-12,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.6966494155545733,
      "clipped_grad_norm": 1.696649432182312,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.020801395177841187
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 11,
    "timing": {
      "total_time": 0.04088005299854558,
      "forward_time": 0.00443039999663597,
      "logprob_time": 0.0039333029999397695,
      "loss_time": 0.009680901996034663,
      "backward_time": 0.008874356004525907,
      "optim_time": 0.01396069100155728
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16852628398495.113,
      "mfu_percent": 10.213714180906129
    },
    "training_dynamics": {
      "loss": -0.12985827028751373,
      "current_logprobs": [
        -17.151912689208984,
        -26.043392181396484
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -16.93936538696289,
        -24.28167724609375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.28985828161239624,
      "kl_div": -28.985828399658203,
      "kl_coef": 0.01,
      "total_loss": -0.12985827028751373,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 3.7063218162949885e-13,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.0524533561207148,
      "clipped_grad_norm": 1.0524532794952393,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.020507272332906723
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 12,
    "timing": {
      "total_time": 0.04076167199673364,
      "forward_time": 0.004519727001024876,
      "logprob_time": 0.003937460001907311,
      "loss_time": 0.009631179003918078,
      "backward_time": 0.008716260999790393,
      "optim_time": 0.013956203001725953
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16519556332289.43,
      "mfu_percent": 10.011852322599655
    },
    "training_dynamics": {
      "loss": -0.11472178995609283,
      "current_logprobs": [
        -25.169214248657227,
        -28.86391830444336
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -24.956666946411133,
        -27.102203369140625
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.27472180128097534,
      "kl_div": -27.472179412841797,
      "kl_coef": 0.01,
      "total_loss": -0.11472178995609283,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.1744025890658705e-12,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.1241435409459044,
      "clipped_grad_norm": 1.1241436004638672,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.02342269755899906
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 13,
    "timing": {
      "total_time": 0.040835539999534376,
      "forward_time": 0.004540376001386903,
      "logprob_time": 0.003926500001398381,
      "loss_time": 0.009586234999005683,
      "backward_time": 0.008831295999698341,
      "optim_time": 0.013950663000287022
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16444427681142.084,
      "mfu_percent": 9.966319806752779
    },
    "training_dynamics": {
      "loss": -0.12609858810901642,
      "current_logprobs": [
        -29.891679763793945,
        -30.840476989746094
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -29.67913246154785,
        -29.07876205444336
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.2860985994338989,
      "kl_div": -28.609859466552734,
      "kl_coef": 0.01,
      "total_loss": -0.12609858810901642,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 6.123902383456392e-13,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.0303446053634995,
      "clipped_grad_norm": 1.0303446054458618,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.026823759078979492
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 14,
    "timing": {
      "total_time": 0.04103404100169428,
      "forward_time": 0.004481075004150625,
      "logprob_time": 0.003920998999092262,
      "loss_time": 0.00987031599652255,
      "backward_time": 0.008798514994850848,
      "optim_time": 0.013962675002403557
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16662047551277.781,
      "mfu_percent": 10.098210637138049
    },
    "training_dynamics": {
      "loss": -0.1385824829339981,
      "current_logprobs": [
        -31.72900390625,
        -32.047210693359375
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -31.516456604003906,
        -30.28549575805664
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.2985824942588806,
      "kl_div": -29.858251571655273,
      "kl_coef": 0.01,
      "total_loss": -0.1385824829339981,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.7185951555094559e-13,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 0.9320947274806876,
      "clipped_grad_norm": 0.9320946335792542,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.029646918177604675
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 15,
    "timing": {
      "total_time": 0.04055786099343095,
      "forward_time": 0.004494239001360256,
      "logprob_time": 0.003908015998604242,
      "loss_time": 0.009514230994682293,
      "backward_time": 0.008679612998093944,
      "optim_time": 0.013961341996036936
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16613243037898.459,
      "mfu_percent": 10.068632144180885
    },
    "training_dynamics": {
      "loss": -0.13975562155246735,
      "current_logprobs": [
        -32.71342086791992,
        -32.88862609863281
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -32.50087356567383,
        -31.126911163330078
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.29975563287734985,
      "kl_div": -29.97556495666504,
      "kl_coef": 0.01,
      "total_loss": -0.13975562155246735,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.9820800003459565e-13,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 0.9736594127292011,
      "clipped_grad_norm": 0.9736593961715698,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.03240463137626648
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 16,
    "timing": {
      "total_time": 0.04060084099910455,
      "forward_time": 0.0044312419995549135,
      "logprob_time": 0.0039167209979495965,
      "loss_time": 0.009602736005035695,
      "backward_time": 0.008688378999067936,
      "optim_time": 0.01396130200009793
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16849426144520.984,
      "mfu_percent": 10.211773420921809
    },
    "training_dynamics": {
      "loss": -0.13273687660694122,
      "current_logprobs": [
        -33.400291442871094,
        -33.499237060546875
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -33.187744140625,
        -31.73752212524414
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.29273688793182373,
      "kl_div": -29.27368927001953,
      "kl_coef": 0.01,
      "total_loss": -0.13273687660694122,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 2.2297134028449045e-13,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.0430680220942403,
      "clipped_grad_norm": 1.0430680513381958,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.03332407400012016
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 17,
    "timing": {
      "total_time": 0.040592345998447854,
      "forward_time": 0.00440909099415876,
      "logprob_time": 0.00394154799869284,
      "loss_time": 0.009483032998105045,
      "backward_time": 0.008816207999188919,
      "optim_time": 0.013942035999207292
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16934076638226.793,
      "mfu_percent": 10.263076750440481
    },
    "training_dynamics": {
      "loss": -0.16425572335720062,
      "current_logprobs": [
        -33.905460357666016,
        -33.97359085083008
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -33.69291305541992,
        -32.211875915527344
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.32425573468208313,
      "kl_div": -32.425575256347656,
      "kl_coef": 0.01,
      "total_loss": -0.16425572335720062,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 8.4059786854742e-15,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 0.9546553248101539,
      "clipped_grad_norm": 0.9546552300453186,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.03152607008814812
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 18,
    "timing": {
      "total_time": 0.04031261400086805,
      "forward_time": 0.004467791004572064,
      "logprob_time": 0.003921260002243798,
      "loss_time": 0.009282147999329027,
      "backward_time": 0.008687076006026473,
      "optim_time": 0.013953877998574171
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16711588506175.322,
      "mfu_percent": 10.128235458288074
    },
    "training_dynamics": {
      "loss": -0.165396586060524,
      "current_logprobs": [
        -34.307132720947266,
        -34.37450408935547
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -34.09458541870117,
        -32.612789154052734
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.3253965973854065,
      "kl_div": -32.5396614074707,
      "kl_coef": 0.01,
      "total_loss": -0.165396586060524,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.0606258574316282e-14,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.5284988382736524,
      "clipped_grad_norm": 1.5284987688064575,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.03160923719406128
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 19,
    "timing": {
      "total_time": 0.04048517600313062,
      "forward_time": 0.004407548003655393,
      "logprob_time": 0.003914667999197263,
      "loss_time": 0.009554184995067772,
      "backward_time": 0.008660658000735566,
      "optim_time": 0.013947636994998902
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16940004904785.525,
      "mfu_percent": 10.266669639263954
    },
    "training_dynamics": {
      "loss": -0.15955622494220734,
      "current_logprobs": [
        -34.66316604614258,
        -34.68745040893555
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -34.450618743896484,
        -32.92573547363281
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.31955623626708984,
      "kl_div": -31.955625534057617,
      "kl_coef": 0.01,
      "total_loss": -0.15955622494220734,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.4453057010205793e-14,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.2627003416608455,
      "clipped_grad_norm": 1.2627003192901611,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.042636435478925705
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 20,
    "timing": {
      "total_time": 0.0406292349944124,
      "forward_time": 0.004405544001201633,
      "logprob_time": 0.003965263000281993,
      "loss_time": 0.009506857000815216,
      "backward_time": 0.008806520003417972,
      "optim_time": 0.013944590995379258
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16947710607279.162,
      "mfu_percent": 10.27133976198737
    },
    "training_dynamics": {
      "loss": -0.1653495579957962,
      "current_logprobs": [
        -34.88506317138672,
        -34.82415008544922
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -34.672515869140625,
        -33.062435150146484
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.3253495693206787,
      "kl_div": -32.53495788574219,
      "kl_coef": 0.01,
      "total_loss": -0.1653495579957962,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.1952986750341996e-14,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.0257158227314611,
      "clipped_grad_norm": 1.0257158279418945,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.045735541731119156
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 21,
    "timing": {
      "total_time": 0.04055818200140493,
      "forward_time": 0.004453453999303747,
      "logprob_time": 0.003928302998247091,
      "loss_time": 0.009323846003098879,
      "backward_time": 0.008873403996403795,
      "optim_time": 0.013978725000924896
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16765388126086.617,
      "mfu_percent": 10.160841288537343
    },
    "training_dynamics": {
      "loss": -0.27313232421875,
      "current_logprobs": [
        -35.17512130737305,
        -35.13875961303711
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -34.96257400512695,
        -33.377044677734375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.4331323504447937,
      "kl_div": -43.313236236572266,
      "kl_coef": 0.01,
      "total_loss": -0.27313232421875,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 5.731219315183991e-15,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.8943210294673736,
      "clipped_grad_norm": 2.8943209648132324,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.13125239312648773
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 22,
    "timing": {
      "total_time": 0.042910954005492385,
      "forward_time": 0.005552444999921136,
      "logprob_time": 0.004195212000922766,
      "loss_time": 0.009967347003112081,
      "backward_time": 0.009214120000251569,
      "optim_time": 0.013981349999085069
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 13447028255311.037,
      "mfu_percent": 8.149714094127901
    },
    "training_dynamics": {
      "loss": -0.18821240961551666,
      "current_logprobs": [
        -35.534549713134766,
        -35.542144775390625
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -35.32200241088867,
        -33.78042984008789
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.34821242094039917,
      "kl_div": -34.82124328613281,
      "kl_coef": 0.01,
      "total_loss": -0.18821240961551666,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.2447484796707293e-15,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 0.8796556940635953,
      "clipped_grad_norm": 0.8796557188034058,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.11938121914863586
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 23,
    "timing": {
      "total_time": 0.040695859999686945,
      "forward_time": 0.004600247993948869,
      "logprob_time": 0.003973727994889487,
      "loss_time": 0.009460239998588804,
      "backward_time": 0.008701373997610062,
      "optim_time": 0.013959729003545363
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16230404295205.889,
      "mfu_percent": 9.836608663761144
    },
    "training_dynamics": {
      "loss": -0.17605538666248322,
      "current_logprobs": [
        -36.02295684814453,
        -36.24253845214844
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -35.81040954589844,
        -34.4808235168457
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.3360553979873657,
      "kl_div": -33.60554122924805,
      "kl_coef": 0.01,
      "total_loss": -0.17605538666248322,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 3.4828944470242235e-15,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.0179663218506798,
      "clipped_grad_norm": 1.017966389656067,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.13132713735103607
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 24,
    "timing": {
      "total_time": 0.04080464299477171,
      "forward_time": 0.004555374005576596,
      "logprob_time": 0.003953170002205297,
      "loss_time": 0.009603297003195621,
      "backward_time": 0.008731269001145847,
      "optim_time": 0.01396096199459862
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16390286441595.795,
      "mfu_percent": 9.933506934300482
    },
    "training_dynamics": {
      "loss": -0.3128198981285095,
      "current_logprobs": [
        -36.648582458496094,
        -36.98985290527344
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -36.43603515625,
        -35.2281379699707
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.4728199243545532,
      "kl_div": -47.2819938659668,
      "kl_coef": 0.01,
      "total_loss": -0.3128198981285095,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 6.908105315065719e-16,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 3.7659599841554527,
      "clipped_grad_norm": 3.7659599781036377,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.1783398687839508
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 25,
    "timing": {
      "total_time": 0.04041995399893494,
      "forward_time": 0.004408989996591117,
      "logprob_time": 0.003922572999726981,
      "loss_time": 0.009366885999043006,
      "backward_time": 0.008777005001320504,
      "optim_time": 0.01394402000005357
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16934464550322.773,
      "mfu_percent": 10.26331184868047
    },
    "training_dynamics": {
      "loss": -0.20172055065631866,
      "current_logprobs": [
        -37.18559646606445,
        -37.417625427246094
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -36.97304916381836,
        -35.65591049194336
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.36172056198120117,
      "kl_div": -36.17205810546875,
      "kl_coef": 0.01,
      "total_loss": -0.20172055065631866,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.5127022703369437e-15,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 0.9157326553269509,
      "clipped_grad_norm": 0.9157326221466064,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.17964844405651093
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 26,
    "timing": {
      "total_time": 0.04105656200408703,
      "forward_time": 0.004568006996123586,
      "logprob_time": 0.003963839997595642,
      "loss_time": 0.00985697099531535,
      "backward_time": 0.008711602000403218,
      "optim_time": 0.013955711998278275
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16344958504520.643,
      "mfu_percent": 9.906035457285238
    },
    "training_dynamics": {
      "loss": -0.20079956948757172,
      "current_logprobs": [
        -38.11115264892578,
        -38.15411376953125
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -37.89860534667969,
        -36.392398834228516
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.3607995808124542,
      "kl_div": -36.079959869384766,
      "kl_coef": 0.01,
      "total_loss": -0.20079956948757172,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 4.496711569724685e-16,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.312332291587135,
      "clipped_grad_norm": 1.312332272529602,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.2544367015361786
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 27,
    "timing": {
      "total_time": 0.04051944000093499,
      "forward_time": 0.00442398800078081,
      "logprob_time": 0.003938082001695875,
      "loss_time": 0.009441764996154234,
      "backward_time": 0.0087586809968343,
      "optim_time": 0.013956353002868127
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16877054093913.057,
      "mfu_percent": 10.22851763267458
    },
    "training_dynamics": {
      "loss": -0.189108207821846,
      "current_logprobs": [
        -41.13486099243164,
        -40.80958557128906
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -40.92231369018555,
        -39.04787063598633
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.3491082191467285,
      "kl_div": -34.910823822021484,
      "kl_coef": 0.01,
      "total_loss": -0.189108207821846,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 9.705054693711517e-16,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 0.9808442646414984,
      "clipped_grad_norm": 0.9808443784713745,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.27042853832244873
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 28,
    "timing": {
      "total_time": 0.040685138999833725,
      "forward_time": 0.0044426829990698025,
      "logprob_time": 0.003949162004573736,
      "loss_time": 0.009659301998908632,
      "backward_time": 0.00867815999663435,
      "optim_time": 0.013955342001281679
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16806034735233.852,
      "mfu_percent": 10.185475597111425
    },
    "training_dynamics": {
      "loss": -0.25232040882110596,
      "current_logprobs": [
        -44.689334869384766,
        -44.29706954956055
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -44.47678756713867,
        -42.53535461425781
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.41232043504714966,
      "kl_div": -41.2320442199707,
      "kl_coef": 0.01,
      "total_loss": -0.25232040882110596,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.283497279009743e-18,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.6228374982000688,
      "clipped_grad_norm": 1.6228374242782593,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.30738160014152527
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 29,
    "timing": {
      "total_time": 0.040661966006155126,
      "forward_time": 0.004446680999535602,
      "logprob_time": 0.003958039000281133,
      "loss_time": 0.009541892002744135,
      "backward_time": 0.008754392998525873,
      "optim_time": 0.013960440999653656
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16790924468788.672,
      "mfu_percent": 10.176317859871922
    },
    "training_dynamics": {
      "loss": -0.20622624456882477,
      "current_logprobs": [
        -48.7515754699707,
        -48.286399841308594
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -48.53902816772461,
        -46.52468490600586
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.3662262558937073,
      "kl_div": -36.62262725830078,
      "kl_coef": 0.01,
      "total_loss": -0.20622624456882477,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.4756936538659496e-16,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 0.9687388353618094,
      "clipped_grad_norm": 0.968738853931427,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.27899956703186035
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 30,
    "timing": {
      "total_time": 0.040600300999358296,
      "forward_time": 0.004496453999308869,
      "logprob_time": 0.003936108005291317,
      "loss_time": 0.00933801200153539,
      "backward_time": 0.008863626004313119,
      "optim_time": 0.013965651000034995
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16605059189191.365,
      "mfu_percent": 10.063672235873556
    },
    "training_dynamics": {
      "loss": -0.2746461033821106,
      "current_logprobs": [
        -52.02806854248047,
        -51.560970306396484
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -51.815521240234375,
        -49.79925537109375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.4346460998058319,
      "kl_div": -43.4646110534668,
      "kl_coef": 0.01,
      "total_loss": -0.2746461033821106,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.4591924263494763e-17,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.7994451740315323,
      "clipped_grad_norm": 1.7994451522827148,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.3331945836544037
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 31,
    "timing": {
      "total_time": 0.04066686499572825,
      "forward_time": 0.004441852004674729,
      "logprob_time": 0.003951647006033454,
      "loss_time": 0.009593879003659822,
      "backward_time": 0.008725027997570578,
      "optim_time": 0.013953998997749295
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16809178856346.777,
      "mfu_percent": 10.187381125058653
    },
    "training_dynamics": {
      "loss": -0.31115567684173584,
      "current_logprobs": [
        -56.171424865722656,
        -55.61756896972656
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -55.95887756347656,
        -53.85585403442383
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.47115570306777954,
      "kl_div": -47.115570068359375,
      "kl_coef": 0.01,
      "total_loss": -0.31115567684173584,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 2.4692140030986873e-19,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 3.5989163126984494,
      "clipped_grad_norm": 3.5989162921905518,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.3434242308139801
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 32,
    "timing": {
      "total_time": 0.04052866699930746,
      "forward_time": 0.004421934994752519,
      "logprob_time": 0.003948700999899302,
      "loss_time": 0.009493651996308472,
      "backward_time": 0.00871846500376705,
      "optim_time": 0.01394545200309949
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16884889734607.846,
      "mfu_percent": 10.233266505822938
    },
    "training_dynamics": {
      "loss": -0.42242997884750366,
      "current_logprobs": [
        -60.44988250732422,
        -59.99161148071289
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -60.237335205078125,
        -58.229896545410156
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.5824300050735474,
      "kl_div": -58.24300003051758,
      "kl_coef": 0.01,
      "total_loss": -0.42242997884750366,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 3.6337842219067806e-25,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 3.9455499096839515,
      "clipped_grad_norm": 3.945549726486206,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.35551419854164124
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 33,
    "timing": {
      "total_time": 0.040676602999155875,
      "forward_time": 0.004550544006633572,
      "logprob_time": 0.00395821900019655,
      "loss_time": 0.009393375003128313,
      "backward_time": 0.008823131000099238,
      "optim_time": 0.013950863001809921
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16407683277242.994,
      "mfu_percent": 9.94405047105636
    },
    "training_dynamics": {
      "loss": -0.35901814699172974,
      "current_logprobs": [
        -65.26200866699219,
        -64.77882385253906
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -65.0494613647461,
        -63.01710891723633
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.5190181732177734,
      "kl_div": -51.90182113647461,
      "kl_coef": 0.01,
      "total_loss": -0.35901814699172974,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.5126414047331108e-18,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.3525103747523668,
      "clipped_grad_norm": 1.3525103330612183,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.3921336233615875
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 34,
    "timing": {
      "total_time": 0.040580664994195104,
      "forward_time": 0.004484381002839655,
      "logprob_time": 0.003937731002224609,
      "loss_time": 0.009438328997930512,
      "backward_time": 0.008756807001191191,
      "optim_time": 0.01396301500062691
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16649763869912.129,
      "mfu_percent": 10.090765981764926
    },
    "training_dynamics": {
      "loss": -0.4846131205558777,
      "current_logprobs": [
        -71.67555236816406,
        -71.08612823486328
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -71.46300506591797,
        -69.32441711425781
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.6446131467819214,
      "kl_div": -64.46131896972656,
      "kl_coef": 0.01,
      "total_loss": -0.4846131205558777,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 3.564099377249786e-27,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.3939384504429926,
      "clipped_grad_norm": 2.3939383029937744,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.41097673773765564
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 35,
    "timing": {
      "total_time": 0.0407832929995493,
      "forward_time": 0.004524477000813931,
      "logprob_time": 0.00396926899702521,
      "loss_time": 0.009542854000756051,
      "backward_time": 0.008789507999608759,
      "optim_time": 0.01395665399468271
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16502213357824.193,
      "mfu_percent": 10.001341428984361
    },
    "training_dynamics": {
      "loss": -0.5783284902572632,
      "current_logprobs": [
        -79.07445526123047,
        -78.10333251953125
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -78.86190795898438,
        -76.34162139892578
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.7383285164833069,
      "kl_div": -73.83285522460938,
      "kl_coef": 0.01,
      "total_loss": -0.5783284902572632,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 9.935006420081346e-33,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.6136085325707386,
      "clipped_grad_norm": 1.6136085987091064,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.43433070182800293
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 36,
    "timing": {
      "total_time": 0.040515251996112056,
      "forward_time": 0.004463142002350651,
      "logprob_time": 0.003957387001719326,
      "loss_time": 0.009424734002095647,
      "backward_time": 0.00871192300110124,
      "optim_time": 0.013957636001578066
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16728996021340.117,
      "mfu_percent": 10.138785467478858
    },
    "training_dynamics": {
      "loss": -0.6657912731170654,
      "current_logprobs": [
        -87.30670166015625,
        -86.65779876708984
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -87.09415435791016,
        -84.89608764648438
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.8257912993431091,
      "kl_div": -82.57913208007812,
      "kl_coef": 0.01,
      "total_loss": -0.6657912731170654,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 7.471609693606948e-36,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 3.4051100389229596,
      "clipped_grad_norm": 3.4051101207733154,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.4306612014770508
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 37,
    "timing": {
      "total_time": 0.04058518300007563,
      "forward_time": 0.004393672003061511,
      "logprob_time": 0.003973808001319412,
      "loss_time": 0.009437067004910205,
      "backward_time": 0.008823832002235577,
      "optim_time": 0.01395626299927244
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16993504464596.857,
      "mfu_percent": 10.299093614907186
    },
    "training_dynamics": {
      "loss": -0.726575493812561,
      "current_logprobs": [
        -94.8696517944336,
        -94.63716125488281
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -94.6571044921875,
        -92.87545013427734
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.8865755200386047,
      "kl_div": -88.65755462646484,
      "kl_coef": 0.01,
      "total_loss": -0.726575493812561,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 3.2473017989000435e-39,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 1.722986744160708,
      "clipped_grad_norm": 1.7229866981506348,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.44055575132369995
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 38,
    "timing": {
      "total_time": 0.04054230199835729,
      "forward_time": 0.004447532002814114,
      "logprob_time": 0.003924786993593443,
      "loss_time": 0.009527766000246629,
      "backward_time": 0.008684211999934632,
      "optim_time": 0.013957565002783667
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16787711646089.89,
      "mfu_percent": 10.174370694599935
    },
    "training_dynamics": {
      "loss": -0.8108264207839966,
      "current_logprobs": [
        -102.99091339111328,
        -102.75447845458984
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -102.77836608886719,
        -100.99276733398438
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -0.9708264470100403,
      "kl_div": -97.08264923095703,
      "kl_coef": 0.01,
      "total_loss": -0.8108264207839966,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.0397634605290143e-42,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.2038036474565756,
      "clipped_grad_norm": 2.203803539276123,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.44079089164733887
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 39,
    "timing": {
      "total_time": 0.040681933001906145,
      "forward_time": 0.0044300900044618174,
      "logprob_time": 0.003953810999519192,
      "loss_time": 0.009564505002344958,
      "backward_time": 0.008783035998931155,
      "optim_time": 0.013949991000117734
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16853807648332.514,
      "mfu_percent": 10.21442887777728
    },
    "training_dynamics": {
      "loss": -0.8730308413505554,
      "current_logprobs": [
        -110.1968002319336,
        -110.07431030273438
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -109.9842529296875,
        -108.3125991821289
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.0330308675765991,
      "kl_div": -103.30308532714844,
      "kl_coef": 0.01,
      "total_loss": -0.8730308413505554,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 1.401298464324817e-45,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 4.931524909681764,
      "clipped_grad_norm": 4.931524753570557,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.4192222058773041
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 40,
    "timing": {
      "total_time": 0.0406603629962774,
      "forward_time": 0.0044668979971902445,
      "logprob_time": 0.00393229099427117,
      "loss_time": 0.009513960001640953,
      "backward_time": 0.008772535999014508,
      "optim_time": 0.013974166002299171
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16714929431333.525,
      "mfu_percent": 10.130260261414257
    },
    "training_dynamics": {
      "loss": -0.9553555846214294,
      "current_logprobs": [
        -114.91561126708984,
        -114.88203430175781
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -114.70306396484375,
        -113.12032318115234
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.1153556108474731,
      "kl_div": -111.53556060791016,
      "kl_coef": 0.01,
      "total_loss": -0.9553555846214294,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.829234602981866,
      "clipped_grad_norm": 2.8292346000671387,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.4112432599067688
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 41,
    "timing": {
      "total_time": 0.04317642000387423,
      "forward_time": 0.005652302002999932,
      "logprob_time": 0.004216590998112224,
      "loss_time": 0.009857211000053212,
      "backward_time": 0.009461913003178779,
      "optim_time": 0.013987712001835462
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 13209464880038.701,
      "mfu_percent": 8.005736290932546
    },
    "training_dynamics": {
      "loss": -0.9615235924720764,
      "current_logprobs": [
        -121.67036437988281,
        -121.71672058105469
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -121.45781707763672,
        -119.95500946044922
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.1215236186981201,
      "kl_div": -112.15235900878906,
      "kl_coef": 0.01,
      "total_loss": -0.9615235924720764,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 4.18901003386556,
      "clipped_grad_norm": 4.189010143280029,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.39146703481674194
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 42,
    "timing": {
      "total_time": 0.04125981200195383,
      "forward_time": 0.004767439000715967,
      "logprob_time": 0.004004304995760322,
      "loss_time": 0.009634504996938631,
      "backward_time": 0.008896207000361755,
      "optim_time": 0.013956893999420572
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 15661214498766.965,
      "mfu_percent": 9.491645150767859
    },
    "training_dynamics": {
      "loss": -1.0233291387557983,
      "current_logprobs": [
        -128.18463134765625,
        -128.2456817626953
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -127.97208404541016,
        -126.48397064208984
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.1833291053771973,
      "kl_div": -118.33291625976562,
      "kl_coef": 0.01,
      "total_loss": -1.0233291387557983,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.177378449715049,
      "clipped_grad_norm": 2.1773784160614014,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.3718736171722412
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 43,
    "timing": {
      "total_time": 0.04049095600203145,
      "forward_time": 0.0044652549986494705,
      "logprob_time": 0.003927001002011821,
      "loss_time": 0.00936520299728727,
      "backward_time": 0.008774419002293143,
      "optim_time": 0.013958166004158556
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16721079719429.756,
      "mfu_percent": 10.133987708745307
    },
    "training_dynamics": {
      "loss": -1.0140774250030518,
      "current_logprobs": [
        -136.83981323242188,
        -136.9544219970703
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -136.62725830078125,
        -135.1927032470703
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.1740773916244507,
      "kl_div": -117.40774536132812,
      "kl_coef": 0.01,
      "total_loss": -1.0140774250030518,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.0277730024049325,
      "clipped_grad_norm": 2.027773141860962,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.35512033104896545
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 44,
    "timing": {
      "total_time": 0.04077230199618498,
      "forward_time": 0.00442025200027274,
      "logprob_time": 0.003993254002125468,
      "loss_time": 0.009658750001108274,
      "backward_time": 0.008740376004425343,
      "optim_time": 0.013959128998976666
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16891318593463.238,
      "mfu_percent": 10.237162783917114
    },
    "training_dynamics": {
      "loss": -1.1768678426742554,
      "current_logprobs": [
        -144.75758361816406,
        -144.87086486816406
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -144.54502868652344,
        -143.10914611816406
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.3368678092956543,
      "kl_div": -133.68678283691406,
      "kl_coef": 0.01,
      "total_loss": -1.1768678426742554,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 4.180021072540602,
      "clipped_grad_norm": 4.180021286010742,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.3234005570411682
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 45,
    "timing": {
      "total_time": 0.04061870599980466,
      "forward_time": 0.004457080998690799,
      "logprob_time": 0.0039042380012688227,
      "loss_time": 0.009572809998644516,
      "backward_time": 0.008727131003979594,
      "optim_time": 0.013957005001429934
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16751745104459.938,
      "mfu_percent": 10.15257279058178
    },
    "training_dynamics": {
      "loss": -1.2027902603149414,
      "current_logprobs": [
        -150.562744140625,
        -150.76889038085938
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -150.35018920898438,
        -149.00717163085938
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.3627902269363403,
      "kl_div": -136.27902221679688,
      "kl_coef": 0.01,
      "total_loss": -1.2027902603149414,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.2370708871520892,
      "clipped_grad_norm": 2.2370707988739014,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.3069134056568146
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 46,
    "timing": {
      "total_time": 0.04049228900112212,
      "forward_time": 0.004414501003338955,
      "logprob_time": 0.003946497003198601,
      "loss_time": 0.009341799006506335,
      "backward_time": 0.008809343999018893,
      "optim_time": 0.01397966700460529
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16913323780768.691,
      "mfu_percent": 10.250499261071933
    },
    "training_dynamics": {
      "loss": -1.3120155334472656,
      "current_logprobs": [
        -156.32525634765625,
        -156.63616943359375
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -156.11270141601562,
        -154.87445068359375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.4720155000686646,
      "kl_div": -147.20155334472656,
      "kl_coef": 0.01,
      "total_loss": -1.3120155334472656,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 3.71673619215969,
      "clipped_grad_norm": 3.716736078262329,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.27848300337791443
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 47,
    "timing": {
      "total_time": 0.040656135002791416,
      "forward_time": 0.004446250000910368,
      "logprob_time": 0.003965361996961292,
      "loss_time": 0.009615840994229075,
      "backward_time": 0.00867625599494204,
      "optim_time": 0.013951984998129774
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16792552102268.79,
      "mfu_percent": 10.177304304405327
    },
    "training_dynamics": {
      "loss": -1.3516017198562622,
      "current_logprobs": [
        -160.96913146972656,
        -161.38839721679688
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -160.75657653808594,
        -159.62667846679688
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.5116016864776611,
      "kl_div": -151.16017150878906,
      "kl_coef": 0.01,
      "total_loss": -1.3516017198562622,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.619329647901218,
      "clipped_grad_norm": 2.6193296909332275,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.2526351809501648
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 48,
    "timing": {
      "total_time": 0.04049963300349191,
      "forward_time": 0.004430932000104804,
      "logprob_time": 0.003913535001629498,
      "loss_time": 0.009509572002571076,
      "backward_time": 0.008690773000125773,
      "optim_time": 0.013954338995972648
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16850604973904.812,
      "mfu_percent": 10.212487862972614
    },
    "training_dynamics": {
      "loss": -1.2930018901824951,
      "current_logprobs": [
        -164.31689453125,
        -164.6737823486328
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -164.10433959960938,
        -162.9120635986328
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.453001856803894,
      "kl_div": -145.30018615722656,
      "kl_coef": 0.01,
      "total_loss": -1.2930018901824951,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.9029859578914325,
      "clipped_grad_norm": 2.9029860496520996,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.22916074097156525
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 49,
    "timing": {
      "total_time": 0.04080692699790234,
      "forward_time": 0.004477488997508772,
      "logprob_time": 0.00391886499710381,
      "loss_time": 0.009563221996359061,
      "backward_time": 0.008880818000761792,
      "optim_time": 0.013966090999019798
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16675392131961.062,
      "mfu_percent": 10.106298261794583
    },
    "training_dynamics": {
      "loss": -1.405562162399292,
      "current_logprobs": [
        -168.66346740722656,
        -168.80807495117188
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -168.45091247558594,
        -167.04635620117188
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.565562129020691,
      "kl_div": -156.55621337890625,
      "kl_coef": 0.01,
      "total_loss": -1.405562162399292,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.859588192179112,
      "clipped_grad_norm": 2.859588384628296,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.20784077048301697
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 50,
    "timing": {
      "total_time": 0.040579741995316,
      "forward_time": 0.004427163999935146,
      "logprob_time": 0.003965572999732103,
      "loss_time": 0.009565596999891568,
      "backward_time": 0.008661939995363355,
      "optim_time": 0.013959007999801543
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16864946679430.387,
      "mfu_percent": 10.221179805715385
    },
    "training_dynamics": {
      "loss": -1.4470582008361816,
      "current_logprobs": [
        -172.14479064941406,
        -172.202880859375
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -171.93223571777344,
        -170.441162109375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.6070581674575806,
      "kl_div": -160.70582580566406,
      "kl_coef": 0.01,
      "total_loss": -1.4470582008361816,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.562607384340744,
      "clipped_grad_norm": 2.5626072883605957,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.18848568201065063
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 51,
    "timing": {
      "total_time": 0.04083585999615025,
      "forward_time": 0.0045023540005786344,
      "logprob_time": 0.0039506539978901856,
      "loss_time": 0.009616461000405252,
      "backward_time": 0.008804416000202764,
      "optim_time": 0.013961532997200266
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16583299489645.713,
      "mfu_percent": 10.050484539179221
    },
    "training_dynamics": {
      "loss": -1.4455666542053223,
      "current_logprobs": [
        -172.9229736328125,
        -172.92388916015625
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -172.71041870117188,
        -171.16217041015625
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.6055666208267212,
      "kl_div": -160.55667114257812,
      "kl_coef": 0.01,
      "total_loss": -1.4455666542053223,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.6305473019636896,
      "clipped_grad_norm": 2.630547285079956,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.17091386020183563
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 52,
    "timing": {
      "total_time": 0.04062901399447583,
      "forward_time": 0.004463892997591756,
      "logprob_time": 0.0038919660000829026,
      "loss_time": 0.009663147997343913,
      "backward_time": 0.008657300997583661,
      "optim_time": 0.013952205001260154
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16726181572963.492,
      "mfu_percent": 10.137079741189996
    },
    "training_dynamics": {
      "loss": -1.4908512830734253,
      "current_logprobs": [
        -174.16104125976562,
        -174.29617309570312
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -173.948486328125,
        -172.53445434570312
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.6508512496948242,
      "kl_div": -165.0851287841797,
      "kl_coef": 0.01,
      "total_loss": -1.4908512830734253,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 5.013851883961831,
      "clipped_grad_norm": 5.013851642608643,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.15497291088104248
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 53,
    "timing": {
      "total_time": 0.040816043998347595,
      "forward_time": 0.004420422003022395,
      "logprob_time": 0.003979308996349573,
      "loss_time": 0.009581385995261371,
      "backward_time": 0.008874156003003009,
      "optim_time": 0.01396028100134572
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16890668978877.973,
      "mfu_percent": 10.236769078107862
    },
    "training_dynamics": {
      "loss": -1.475389838218689,
      "current_logprobs": [
        -174.9676513671875,
        -175.205810546875
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -174.75509643554688,
        -173.444091796875
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.635389804840088,
      "kl_div": -163.5389862060547,
      "kl_coef": 0.01,
      "total_loss": -1.475389838218689,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.752274995569123,
      "clipped_grad_norm": 2.752274990081787,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.1404998004436493
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 54,
    "timing": {
      "total_time": 0.040536220993089955,
      "forward_time": 0.004477258000406437,
      "logprob_time": 0.003915458997653332,
      "loss_time": 0.009508950002782512,
      "backward_time": 0.008663492000778206,
      "optim_time": 0.013970590000099037
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16676252472656.738,
      "mfu_percent": 10.106819680398022
    },
    "training_dynamics": {
      "loss": -1.484930396080017,
      "current_logprobs": [
        -176.02235412597656,
        -176.3356475830078
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -175.80979919433594,
        -174.5739288330078
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.644930362701416,
      "kl_div": -164.4930419921875,
      "kl_coef": 0.01,
      "total_loss": -1.484930396080017,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.6011585591395954,
      "clipped_grad_norm": 2.601158618927002,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.12736071646213531
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 55,
    "timing": {
      "total_time": 0.04061617099796422,
      "forward_time": 0.004411165005876683,
      "logprob_time": 0.003916702000424266,
      "loss_time": 0.009486088005360216,
      "backward_time": 0.0088409239979228,
      "optim_time": 0.013960740994662046
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16926114688643.611,
      "mfu_percent": 10.258251326450674
    },
    "training_dynamics": {
      "loss": -1.5109553337097168,
      "current_logprobs": [
        -176.42684936523438,
        -176.8274688720703
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -176.21429443359375,
        -175.0657501220703
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.6709553003311157,
      "kl_div": -167.0955352783203,
      "kl_coef": 0.01,
      "total_loss": -1.5109553337097168,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.6644891406130933,
      "clipped_grad_norm": 2.6644890308380127,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.11543871462345123
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 56,
    "timing": {
      "total_time": 0.04050686700065853,
      "forward_time": 0.0044479429998318665,
      "logprob_time": 0.003966304000641685,
      "loss_time": 0.009467674004554283,
      "backward_time": 0.00867445299809333,
      "optim_time": 0.013950031003332697
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16786160434794.762,
      "mfu_percent": 10.17343056654228
    },
    "training_dynamics": {
      "loss": -1.5126816034317017,
      "current_logprobs": [
        -177.42955017089844,
        -177.7816162109375
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -177.2169952392578,
        -176.0198974609375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.6726815700531006,
      "kl_div": -167.26815795898438,
      "kl_coef": 0.01,
      "total_loss": -1.5126816034317017,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5631862569986503,
      "clipped_grad_norm": 2.563185930252075,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.10462307184934616
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 57,
    "timing": {
      "total_time": 0.04059592199337203,
      "forward_time": 0.004441651006345637,
      "logprob_time": 0.003961034002713859,
      "loss_time": 0.009544476997689344,
      "backward_time": 0.008692066003277432,
      "optim_time": 0.013956253002106678
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16809939523238.143,
      "mfu_percent": 10.187842135295844
    },
    "training_dynamics": {
      "loss": -1.5477476119995117,
      "current_logprobs": [
        -178.5829620361328,
        -178.79693603515625
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -178.3704071044922,
        -177.03521728515625
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7077475786209106,
      "kl_div": -170.77476501464844,
      "kl_coef": 0.01,
      "total_loss": -1.5477476119995117,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.6077289488943225,
      "clipped_grad_norm": 2.607728958129883,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.09481166303157806
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 58,
    "timing": {
      "total_time": 0.040746573999058455,
      "forward_time": 0.00445209100143984,
      "logprob_time": 0.003940627000702079,
      "loss_time": 0.009668077997048385,
      "backward_time": 0.008719778001250234,
      "optim_time": 0.013965530000859872
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16770520812771.604,
      "mfu_percent": 10.163952007740367
    },
    "training_dynamics": {
      "loss": -1.5004048347473145,
      "current_logprobs": [
        -179.1988067626953,
        -179.27976989746094
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -178.9862518310547,
        -177.51805114746094
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.6604048013687134,
      "kl_div": -166.0404815673828,
      "kl_coef": 0.01,
      "total_loss": -1.5004048347473145,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.6038186374724406,
      "clipped_grad_norm": 2.603818655014038,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.08591409027576447
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 59,
    "timing": {
      "total_time": 0.040473173998179846,
      "forward_time": 0.00442539100185968,
      "logprob_time": 0.003930818005756009,
      "loss_time": 0.009501016000285745,
      "backward_time": 0.008640891006507445,
      "optim_time": 0.013974576999316923
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16871703487584.268,
      "mfu_percent": 10.225274840960163
    },
    "training_dynamics": {
      "loss": -1.554872989654541,
      "current_logprobs": [
        -179.67242431640625,
        -179.6190185546875
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -179.45986938476562,
        -177.8572998046875
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.71487295627594,
      "kl_div": -171.4873046875,
      "kl_coef": 0.01,
      "total_loss": -1.554872989654541,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5581279773754497,
      "clipped_grad_norm": 2.5581278800964355,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.07784667611122131
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 60,
    "timing": {
      "total_time": 0.04327063600067049,
      "forward_time": 0.005413175000285264,
      "logprob_time": 0.00417127700347919,
      "loss_time": 0.010042175999842584,
      "backward_time": 0.009641197000746615,
      "optim_time": 0.01400236900371965
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 13792992983981.74,
      "mfu_percent": 8.359389687261661
    },
    "training_dynamics": {
      "loss": -1.554072618484497,
      "current_logprobs": [
        -180.0480194091797,
        -179.89866638183594
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -179.83546447753906,
        -178.13694763183594
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.714072585105896,
      "kl_div": -171.40725708007812,
      "kl_coef": 0.01,
      "total_loss": -1.554072618484497,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.494308276105047,
      "clipped_grad_norm": 2.4943082332611084,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.07053041458129883
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 61,
    "timing": {
      "total_time": 0.04139836000103969,
      "forward_time": 0.004692930000601336,
      "logprob_time": 0.004013411999039818,
      "loss_time": 0.0098098119997303,
      "backward_time": 0.00890930099558318,
      "optim_time": 0.013972263004689012
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 15909865433840.441,
      "mfu_percent": 9.642342687176026
    },
    "training_dynamics": {
      "loss": -1.567690372467041,
      "current_logprobs": [
        -180.5115203857422,
        -180.29161071777344
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -180.29896545410156,
        -178.52989196777344
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.72769033908844,
      "kl_div": -172.76904296875,
      "kl_coef": 0.01,
      "total_loss": -1.567690372467041,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.545082498770304,
      "clipped_grad_norm": 2.5450823307037354,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.06389906257390976
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 62,
    "timing": {
      "total_time": 0.04063399299775483,
      "forward_time": 0.004436000999703538,
      "logprob_time": 0.0039137459971243516,
      "loss_time": 0.009605822000594344,
      "backward_time": 0.008712924995052163,
      "optim_time": 0.013965039004688151
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16831349858800.723,
      "mfu_percent": 10.200818096242863
    },
    "training_dynamics": {
      "loss": -1.5578771829605103,
      "current_logprobs": [
        -181.16140747070312,
        -180.94044494628906
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -180.9488525390625,
        -179.17872619628906
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7178771495819092,
      "kl_div": -171.7877197265625,
      "kl_coef": 0.01,
      "total_loss": -1.5578771829605103,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5582087345676987,
      "clipped_grad_norm": 2.558208703994751,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.057888228446245193
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 63,
    "timing": {
      "total_time": 0.040897405000578146,
      "forward_time": 0.004503145995840896,
      "logprob_time": 0.003944072996091563,
      "loss_time": 0.009670502004155423,
      "backward_time": 0.008826737001072615,
      "optim_time": 0.013952495995908976
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16580382885422.666,
      "mfu_percent": 10.048716900256162
    },
    "training_dynamics": {
      "loss": -1.56974458694458,
      "current_logprobs": [
        -181.70458984375,
        -181.5399627685547
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -181.49203491210938,
        -179.7782440185547
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.729744553565979,
      "kl_div": -172.97445678710938,
      "kl_coef": 0.01,
      "total_loss": -1.56974458694458,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5296871967394683,
      "clipped_grad_norm": 2.52968692779541,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.05244100093841553
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 64,
    "timing": {
      "total_time": 0.0404139629972633,
      "forward_time": 0.004406735999509692,
      "logprob_time": 0.00392379400000209,
      "loss_time": 0.00942446300177835,
      "backward_time": 0.00870388800103683,
      "optim_time": 0.013954519999970216
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16943126343013.82,
      "mfu_percent": 10.268561420008377
    },
    "training_dynamics": {
      "loss": -1.573093295097351,
      "current_logprobs": [
        -182.1644287109375,
        -182.08447265625
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -181.95187377929688,
        -180.32275390625
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.73309326171875,
      "kl_div": -173.309326171875,
      "kl_coef": 0.01,
      "total_loss": -1.573093295097351,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5489551720427155,
      "clipped_grad_norm": 2.548955202102661,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.04750470072031021
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 65,
    "timing": {
      "total_time": 0.04220465599792078,
      "forward_time": 0.0045392339961836115,
      "logprob_time": 0.003943923002225347,
      "loss_time": 0.010957515005429741,
      "backward_time": 0.008784958998148795,
      "optim_time": 0.013978564005810767
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16448564859792.229,
      "mfu_percent": 9.968827187752865
    },
    "training_dynamics": {
      "loss": -1.6009842157363892,
      "current_logprobs": [
        -182.55479431152344,
        -182.56512451171875
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -182.3422393798828,
        -180.80340576171875
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.760984182357788,
      "kl_div": -176.09841918945312,
      "kl_coef": 0.01,
      "total_loss": -1.6009842157363892,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.563946592682823,
      "clipped_grad_norm": 2.563946485519409,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.043031588196754456
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 66,
    "timing": {
      "total_time": 0.040611542004626244,
      "forward_time": 0.004415814000822138,
      "logprob_time": 0.0039585089980391786,
      "loss_time": 0.009549886999593582,
      "backward_time": 0.008725969004444778,
      "optim_time": 0.013960881995444652
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16908294775572.307,
      "mfu_percent": 10.24745137913473
    },
    "training_dynamics": {
      "loss": -1.601619839668274,
      "current_logprobs": [
        -182.75181579589844,
        -182.8279571533203
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -182.5392608642578,
        -181.0662384033203
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7616198062896729,
      "kl_div": -176.1619873046875,
      "kl_coef": 0.01,
      "total_loss": -1.601619839668274,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.551495751444644,
      "clipped_grad_norm": 2.5514957904815674,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.03897704556584358
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 67,
    "timing": {
      "total_time": 0.040938261001429055,
      "forward_time": 0.0045807110000168905,
      "logprob_time": 0.003950104000978172,
      "loss_time": 0.009589681998477317,
      "backward_time": 0.008858777000568807,
      "optim_time": 0.013958527000795584
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16299627896133.307,
      "mfu_percent": 9.878562361292913
    },
    "training_dynamics": {
      "loss": -1.5868542194366455,
      "current_logprobs": [
        -182.916259765625,
        -183.01463317871094
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -182.70370483398438,
        -181.25291442871094
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7468541860580444,
      "kl_div": -174.6854248046875,
      "kl_coef": 0.01,
      "total_loss": -1.5868542194366455,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.594936353291839,
      "clipped_grad_norm": 2.5949363708496094,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.03530360013246536
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 68,
    "timing": {
      "total_time": 0.040353299998969305,
      "forward_time": 0.0044125070053269155,
      "logprob_time": 0.003935838001780212,
      "loss_time": 0.009392252999532502,
      "backward_time": 0.008659325001644902,
      "optim_time": 0.013952897003036924
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16920966858491.883,
      "mfu_percent": 10.255131429389019
    },
    "training_dynamics": {
      "loss": -1.5930789709091187,
      "current_logprobs": [
        -183.0637664794922,
        -183.1735382080078
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -182.85121154785156,
        -181.4118194580078
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7530789375305176,
      "kl_div": -175.30789184570312,
      "kl_coef": 0.01,
      "total_loss": -1.5930789709091187,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.6195637262274025,
      "clipped_grad_norm": 2.6195640563964844,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.031976860016584396
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 69,
    "timing": {
      "total_time": 0.040670592003152706,
      "forward_time": 0.004452532004506793,
      "logprob_time": 0.003953710998757742,
      "loss_time": 0.009441365000384394,
      "backward_time": 0.008869696997862775,
      "optim_time": 0.013952806002635043
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16768859768874.479,
      "mfu_percent": 10.16294531446938
    },
    "training_dynamics": {
      "loss": -1.600412130355835,
      "current_logprobs": [
        -183.44898986816406,
        -183.53298950195312
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -183.23643493652344,
        -181.77127075195312
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7604120969772339,
      "kl_div": -176.0412139892578,
      "kl_coef": 0.01,
      "total_loss": -1.600412130355835,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5609126782065568,
      "clipped_grad_norm": 2.560912609100342,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.028966454789042473
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 70,
    "timing": {
      "total_time": 0.04047146000084467,
      "forward_time": 0.00445550699805608,
      "logprob_time": 0.003981722999014892,
      "loss_time": 0.009361636002722662,
      "backward_time": 0.008722483005840331,
      "optim_time": 0.013949660999060143
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16757663007279.654,
      "mfu_percent": 10.156159398351306
    },
    "training_dynamics": {
      "loss": -1.5799490213394165,
      "current_logprobs": [
        -183.8408966064453,
        -183.8783416748047
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -183.6283416748047,
        -182.1166229248047
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7399489879608154,
      "kl_div": -173.99490356445312,
      "kl_coef": 0.01,
      "total_loss": -1.5799490213394165,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.510491724795159,
      "clipped_grad_norm": 2.510491371154785,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.026242319494485855
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 71,
    "timing": {
      "total_time": 0.04061248300422449,
      "forward_time": 0.004558469001494814,
      "logprob_time": 0.003936308996344451,
      "loss_time": 0.009465398994507268,
      "backward_time": 0.008705942003871314,
      "optim_time": 0.013945893995696679
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16379158172517.178,
      "mfu_percent": 9.92676252879829
    },
    "training_dynamics": {
      "loss": -1.615419626235962,
      "current_logprobs": [
        -184.09666442871094,
        -184.10191345214844
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -183.8841094970703,
        -182.34019470214844
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7754195928573608,
      "kl_div": -177.54196166992188,
      "kl_coef": 0.01,
      "total_loss": -1.615419626235962,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.543053701095746,
      "clipped_grad_norm": 2.54305362701416,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.023775480687618256
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 72,
    "timing": {
      "total_time": 0.04055275199789321,
      "forward_time": 0.004456319002201781,
      "logprob_time": 0.003953840998292435,
      "loss_time": 0.009485257003689185,
      "backward_time": 0.008697766999830492,
      "optim_time": 0.01395899800263578
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16754609524836.535,
      "mfu_percent": 10.154308802931233
    },
    "training_dynamics": {
      "loss": -1.5699341297149658,
      "current_logprobs": [
        -184.30172729492188,
        -184.26898193359375
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -184.08917236328125,
        -182.50726318359375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7299340963363647,
      "kl_div": -172.993408203125,
      "kl_coef": 0.01,
      "total_loss": -1.5699341297149658,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.6429275058058943,
      "clipped_grad_norm": 2.642927646636963,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.021551374346017838
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 73,
    "timing": {
      "total_time": 0.04038534999563126,
      "forward_time": 0.004407778003951535,
      "logprob_time": 0.003936789005820174,
      "loss_time": 0.00931925700569991,
      "backward_time": 0.008756276001804508,
      "optim_time": 0.013964778998342808
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16939120965952.568,
      "mfu_percent": 10.266133918759133
    },
    "training_dynamics": {
      "loss": -1.563942313194275,
      "current_logprobs": [
        -184.40736389160156,
        -184.34637451171875
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -184.19480895996094,
        -182.58465576171875
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7239422798156738,
      "kl_div": -172.39422607421875,
      "kl_coef": 0.01,
      "total_loss": -1.563942313194275,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.9934754444015272,
      "clipped_grad_norm": 2.9934754371643066,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.019576314836740494
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 74,
    "timing": {
      "total_time": 0.040596193000965286,
      "forward_time": 0.004516902001341805,
      "logprob_time": 0.003952778999519069,
      "loss_time": 0.009528035996481776,
      "backward_time": 0.008636261001811363,
      "optim_time": 0.013961763004772365
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16529888135235.193,
      "mfu_percent": 10.018114021354663
    },
    "training_dynamics": {
      "loss": -1.6314289569854736,
      "current_logprobs": [
        -184.5007781982422,
        -184.4232940673828
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -184.28822326660156,
        -182.6615753173828
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7914289236068726,
      "kl_div": -179.1428985595703,
      "kl_coef": 0.01,
      "total_loss": -1.6314289569854736,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.551527239844306,
      "clipped_grad_norm": 2.551527500152588,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.017748204991221428
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 75,
    "timing": {
      "total_time": 0.04043014300259529,
      "forward_time": 0.004430410001077689,
      "logprob_time": 0.003939593996619806,
      "loss_time": 0.009384138000314124,
      "backward_time": 0.008727020998776425,
      "optim_time": 0.01394852899829857
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16852590343069.41,
      "mfu_percent": 10.213691117011765
    },
    "training_dynamics": {
      "loss": -1.6177040338516235,
      "current_logprobs": [
        -184.57374572753906,
        -184.4938507080078
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -184.36119079589844,
        -182.7321319580078
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7777040004730225,
      "kl_div": -177.77040100097656,
      "kl_coef": 0.01,
      "total_loss": -1.6177040338516235,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5425153733325487,
      "clipped_grad_norm": 2.542515277862549,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.01609661430120468
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 76,
    "timing": {
      "total_time": 0.040577497995400336,
      "forward_time": 0.004483379998418968,
      "logprob_time": 0.003959351000958122,
      "loss_time": 0.00941757000691723,
      "backward_time": 0.008765794002101757,
      "optim_time": 0.01395096300257137
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16653481263316.89,
      "mfu_percent": 10.093018947464783
    },
    "training_dynamics": {
      "loss": -1.6249265670776367,
      "current_logprobs": [
        -184.74595642089844,
        -184.66888427734375
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -184.5334014892578,
        -182.90716552734375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7849265336990356,
      "kl_div": -178.49266052246094,
      "kl_coef": 0.01,
      "total_loss": -1.6249265670776367,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5165593108187476,
      "clipped_grad_norm": 2.51655912399292,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.014608088880777359
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 77,
    "timing": {
      "total_time": 0.04048719000275014,
      "forward_time": 0.004410362998896744,
      "logprob_time": 0.003950744998292066,
      "loss_time": 0.009395810004207306,
      "backward_time": 0.008777184993959963,
      "optim_time": 0.013952626002719626
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16929192635317.6,
      "mfu_percent": 10.260116748677333
    },
    "training_dynamics": {
      "loss": -1.610927939414978,
      "current_logprobs": [
        -185.0042724609375,
        -184.9336700439453
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -184.79171752929688,
        -183.1719512939453
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.770927906036377,
      "kl_div": -177.09278869628906,
      "kl_coef": 0.01,
      "total_loss": -1.610927939414978,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.541431464226592,
      "clipped_grad_norm": 2.5414316654205322,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.013263748027384281
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 78,
    "timing": {
      "total_time": 0.040554995997808874,
      "forward_time": 0.004442512996320147,
      "logprob_time": 0.003946818003896624,
      "loss_time": 0.009450292003748473,
      "backward_time": 0.00875218800501898,
      "optim_time": 0.013962605000415351
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16806677855944.621,
      "mfu_percent": 10.185865367239165
    },
    "training_dynamics": {
      "loss": -1.6211328506469727,
      "current_logprobs": [
        -185.254638671875,
        -185.20289611816406
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -185.04208374023438,
        -183.44117736816406
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7811328172683716,
      "kl_div": -178.11328125,
      "kl_coef": 0.01,
      "total_loss": -1.6211328506469727,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.55600543985367,
      "clipped_grad_norm": 2.5560054779052734,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.012052028439939022
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 79,
    "timing": {
      "total_time": 0.04278489799617091,
      "forward_time": 0.00509385799523443,
      "logprob_time": 0.004137042000365909,
      "loss_time": 0.010055901999294292,
      "backward_time": 0.009496717997535598,
      "optim_time": 0.01400096600264078
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 14657629810224.777,
      "mfu_percent": 8.883412006196835
    },
    "training_dynamics": {
      "loss": -1.6185073852539062,
      "current_logprobs": [
        -185.3052520751953,
        -185.28038024902344
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -185.0926971435547,
        -183.51866149902344
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7785073518753052,
      "kl_div": -177.85073852539062,
      "kl_coef": 0.01,
      "total_loss": -1.6185073852539062,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5560789324754256,
      "clipped_grad_norm": 2.5560789108276367,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.01096363551914692
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 80,
    "timing": {
      "total_time": 0.04168226000183495,
      "forward_time": 0.0047181980044115335,
      "logprob_time": 0.004115973002626561,
      "loss_time": 0.009923716002958827,
      "backward_time": 0.008947984002588782,
      "optim_time": 0.013975808993563987
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 15824661180007.49,
      "mfu_percent": 9.590703745459086
    },
    "training_dynamics": {
      "loss": -1.6242740154266357,
      "current_logprobs": [
        -185.3506622314453,
        -185.3511962890625
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -185.1381072998047,
        -183.5894775390625
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7842739820480347,
      "kl_div": -178.42739868164062,
      "kl_coef": 0.01,
      "total_loss": -1.6242740154266357,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5633015700157102,
      "clipped_grad_norm": 2.5633018016815186,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.009984368458390236
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 81,
    "timing": {
      "total_time": 0.04062431499914965,
      "forward_time": 0.004439607997483108,
      "logprob_time": 0.0039248760003829375,
      "loss_time": 0.009532394004054368,
      "backward_time": 0.008778837996942457,
      "optim_time": 0.013948188003269024
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16817675083549.78,
      "mfu_percent": 10.192530353666534
    },
    "training_dynamics": {
      "loss": -1.6172834634780884,
      "current_logprobs": [
        -185.5084228515625,
        -185.5221710205078
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -185.29586791992188,
        -183.7604522705078
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7772834300994873,
      "kl_div": -177.7283477783203,
      "kl_coef": 0.01,
      "total_loss": -1.6172834634780884,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.531596148237399,
      "clipped_grad_norm": 2.5315961837768555,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.009114355780184269
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 82,
    "timing": {
      "total_time": 0.040479825001966674,
      "forward_time": 0.00446724899666151,
      "logprob_time": 0.003967736993217841,
      "loss_time": 0.009267129004001617,
      "backward_time": 0.008809514998574741,
      "optim_time": 0.013967633996799123
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16713616110451.473,
      "mfu_percent": 10.12946430936453
    },
    "training_dynamics": {
      "loss": -1.639889121055603,
      "current_logprobs": [
        -185.6268768310547,
        -185.64883422851562
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -185.41432189941406,
        -183.88711547851562
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.799889087677002,
      "kl_div": -179.98890686035156,
      "kl_coef": 0.01,
      "total_loss": -1.639889121055603,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5670111216993465,
      "clipped_grad_norm": 2.5670111179351807,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.008317714557051659
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 83,
    "timing": {
      "total_time": 0.04050836900569266,
      "forward_time": 0.004421103003551252,
      "logprob_time": 0.003914526998414658,
      "loss_time": 0.009530210998491384,
      "backward_time": 0.008680915001605172,
      "optim_time": 0.013961101998575032
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16888067240239.871,
      "mfu_percent": 10.235192266812042
    },
    "training_dynamics": {
      "loss": -1.6365699768066406,
      "current_logprobs": [
        -185.86489868164062,
        -185.89015197753906
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -185.65234375,
        -184.12843322753906
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7965699434280396,
      "kl_div": -179.65699768066406,
      "kl_coef": 0.01,
      "total_loss": -1.6365699768066406,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.605469781128856,
      "clipped_grad_norm": 2.6054697036743164,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.007609217893332243
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 84,
    "timing": {
      "total_time": 0.04057332999946084,
      "forward_time": 0.004435299000761006,
      "logprob_time": 0.00395793800271349,
      "loss_time": 0.009459488996071741,
      "backward_time": 0.00875837999774376,
      "optim_time": 0.013961673001176678
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16834013848263.49,
      "mfu_percent": 10.202432635311206
    },
    "training_dynamics": {
      "loss": -1.643873929977417,
      "current_logprobs": [
        -186.2342987060547,
        -186.2526397705078
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -186.02174377441406,
        -184.4909210205078
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.803873896598816,
      "kl_div": -180.38739013671875,
      "kl_coef": 0.01,
      "total_loss": -1.643873929977417,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5862186187693714,
      "clipped_grad_norm": 2.5862185955047607,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0069718430750072
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 85,
    "timing": {
      "total_time": 0.040611049997096416,
      "forward_time": 0.004528794001089409,
      "logprob_time": 0.003948030003812164,
      "loss_time": 0.009400599003129173,
      "backward_time": 0.008789637999143451,
      "optim_time": 0.013943548998213373
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16486482887505.918,
      "mfu_percent": 9.991807810609647
    },
    "training_dynamics": {
      "loss": -1.6469537019729614,
      "current_logprobs": [
        -186.4555206298828,
        -186.46548461914062
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -186.2429656982422,
        -184.70376586914062
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8069536685943604,
      "kl_div": -180.69537353515625,
      "kl_coef": 0.01,
      "total_loss": -1.6469537019729614,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.520648347004613,
      "clipped_grad_norm": 2.52064847946167,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.006410377565771341
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 86,
    "timing": {
      "total_time": 0.04064553500211332,
      "forward_time": 0.004402427999593783,
      "logprob_time": 0.003950975995394401,
      "loss_time": 0.009600833000149578,
      "backward_time": 0.008730367000680417,
      "optim_time": 0.013960481002868619
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16959706054679.217,
      "mfu_percent": 10.278609730108617
    },
    "training_dynamics": {
      "loss": -1.6344436407089233,
      "current_logprobs": [
        -186.6028594970703,
        -186.59988403320312
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -186.3903045654297,
        -184.83816528320312
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7944436073303223,
      "kl_div": -179.44436645507812,
      "kl_coef": 0.01,
      "total_loss": -1.6344436407089233,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5523272869699762,
      "clipped_grad_norm": 2.5523273944854736,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.005911181680858135
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 87,
    "timing": {
      "total_time": 0.04075007000210462,
      "forward_time": 0.004527582001173869,
      "logprob_time": 0.003987994998169597,
      "loss_time": 0.009472161997109652,
      "backward_time": 0.008800198003882542,
      "optim_time": 0.013961552998807747
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16490896195947.826,
      "mfu_percent": 9.994482542998682
    },
    "training_dynamics": {
      "loss": -1.6537847518920898,
      "current_logprobs": [
        -186.7747039794922,
        -186.76759338378906
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -186.56214904785156,
        -185.00587463378906
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8137847185134888,
      "kl_div": -181.37847900390625,
      "kl_coef": 0.01,
      "total_loss": -1.6537847518920898,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.568544202348921,
      "clipped_grad_norm": 2.5685441493988037,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.005482334177941084
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 88,
    "timing": {
      "total_time": 0.04046004900010303,
      "forward_time": 0.004439898002601694,
      "logprob_time": 0.003962777002016082,
      "loss_time": 0.009372537002491299,
      "backward_time": 0.00871993799955817,
      "optim_time": 0.013964388002932537
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16816576587175.744,
      "mfu_percent": 10.19186459828833
    },
    "training_dynamics": {
      "loss": -1.6484935283660889,
      "current_logprobs": [
        -186.9792022705078,
        -186.9657745361328
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -186.7666473388672,
        -185.2040557861328
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8084934949874878,
      "kl_div": -180.84934997558594,
      "kl_coef": 0.01,
      "total_loss": -1.6484935283660889,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.535202205094769,
      "clipped_grad_norm": 2.5352022647857666,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.005102999042719603
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 89,
    "timing": {
      "total_time": 0.04073039299692027,
      "forward_time": 0.004414099996211007,
      "logprob_time": 0.003962998001952656,
      "loss_time": 0.00954361599724507,
      "backward_time": 0.008853867999278009,
      "optim_time": 0.01395526099804556
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16914860303140.002,
      "mfu_percent": 10.251430486751516
    },
    "training_dynamics": {
      "loss": -1.6592367887496948,
      "current_logprobs": [
        -187.13751220703125,
        -187.1219482421875
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -186.92495727539062,
        -185.3602294921875
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8192367553710938,
      "kl_div": -181.92367553710938,
      "kl_coef": 0.01,
      "total_loss": -1.6592367887496948,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5700290453813754,
      "clipped_grad_norm": 2.5700290203094482,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0047591859474778175
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 90,
    "timing": {
      "total_time": 0.04048216999944998,
      "forward_time": 0.004438456002390012,
      "logprob_time": 0.003964711002481636,
      "loss_time": 0.009430744998098817,
      "backward_time": 0.008691846000147052,
      "optim_time": 0.013955941998574417
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16822040087768.16,
      "mfu_percent": 10.195175810768582
    },
    "training_dynamics": {
      "loss": -1.6184360980987549,
      "current_logprobs": [
        -187.2906494140625,
        -187.2735137939453
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.07809448242188,
        -185.5117950439453
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.7784360647201538,
      "kl_div": -177.84361267089844,
      "kl_coef": 0.01,
      "total_loss": -1.6184360980987549,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.599700924830547,
      "clipped_grad_norm": 2.599700927734375,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.004470570012927055
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 91,
    "timing": {
      "total_time": 0.04033976500068093,
      "forward_time": 0.00440109600458527,
      "logprob_time": 0.003916702000424266,
      "loss_time": 0.009311753005022183,
      "backward_time": 0.008752328001719434,
      "optim_time": 0.01395745600166265
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16964838922443.781,
      "mfu_percent": 10.281720559056836
    },
    "training_dynamics": {
      "loss": -1.6580945253372192,
      "current_logprobs": [
        -187.46617126464844,
        -187.46200561523438
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.2536163330078,
        -185.70028686523438
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8180944919586182,
      "kl_div": -181.8094482421875,
      "kl_coef": 0.01,
      "total_loss": -1.6580945253372192,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.54877418415245,
      "clipped_grad_norm": 2.548774242401123,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.004200897179543972
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 92,
    "timing": {
      "total_time": 0.040431324996461626,
      "forward_time": 0.004438977004610933,
      "logprob_time": 0.003939082998840604,
      "loss_time": 0.009388555998157244,
      "backward_time": 0.008711501999641769,
      "optim_time": 0.013952736000646837
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16820065686856.184,
      "mfu_percent": 10.193979204155262
    },
    "training_dynamics": {
      "loss": -1.6504977941513062,
      "current_logprobs": [
        -187.64508056640625,
        -187.65145874023438
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.43252563476562,
        -185.88973999023438
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.810497760772705,
      "kl_div": -181.04977416992188,
      "kl_coef": 0.01,
      "total_loss": -1.6504977941513062,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5698467054955723,
      "clipped_grad_norm": 2.5698471069335938,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.003988512791693211
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 93,
    "timing": {
      "total_time": 0.04043404099502368,
      "forward_time": 0.0044503079989226535,
      "logprob_time": 0.003944604002754204,
      "loss_time": 0.009404656004335266,
      "backward_time": 0.00868631499906769,
      "optim_time": 0.013947685998573434
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16777239871504.377,
      "mfu_percent": 10.168024164548108
    },
    "training_dynamics": {
      "loss": -1.647025465965271,
      "current_logprobs": [
        -187.77003479003906,
        -187.78817749023438
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.55747985839844,
        -186.02645874023438
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.80702543258667,
      "kl_div": -180.70254516601562,
      "kl_coef": 0.01,
      "total_loss": -1.647025465965271,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5939702616359903,
      "clipped_grad_norm": 2.59397029876709,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0038054240867495537
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 94,
    "timing": {
      "total_time": 0.04050438199919881,
      "forward_time": 0.004454516005353071,
      "logprob_time": 0.003958419001719449,
      "loss_time": 0.009431656995730009,
      "backward_time": 0.008691896000527777,
      "optim_time": 0.013967464001325425
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16761391071504.758,
      "mfu_percent": 10.158418831215005
    },
    "training_dynamics": {
      "loss": -1.6470425128936768,
      "current_logprobs": [
        -187.85096740722656,
        -187.87347412109375
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.63841247558594,
        -186.11175537109375
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8070424795150757,
      "kl_div": -180.70425415039062,
      "kl_coef": 0.01,
      "total_loss": -1.6470425128936768,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5917097985563293,
      "clipped_grad_norm": 2.591709613800049,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0036423499695956707
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 95,
    "timing": {
      "total_time": 0.04048737999983132,
      "forward_time": 0.004408640001202002,
      "logprob_time": 0.003950364000047557,
      "loss_time": 0.009453398000914603,
      "backward_time": 0.00872797299962258,
      "optim_time": 0.0139465440006461
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16935808952339.752,
      "mfu_percent": 10.264126637781668
    },
    "training_dynamics": {
      "loss": -1.6686865091323853,
      "current_logprobs": [
        -187.7700653076172,
        -187.79580688476562
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.55751037597656,
        -186.03408813476562
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8286864757537842,
      "kl_div": -182.86865234375,
      "kl_coef": 0.01,
      "total_loss": -1.6686865091323853,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.6026684782191656,
      "clipped_grad_norm": 2.602668523788452,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0034703200217336416
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 96,
    "timing": {
      "total_time": 0.04037334700115025,
      "forward_time": 0.004469833998882677,
      "logprob_time": 0.003950443999201525,
      "loss_time": 0.009341047996713314,
      "backward_time": 0.008660025996505283,
      "optim_time": 0.013951533997897059
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16703950262730.943,
      "mfu_percent": 10.123606219836935
    },
    "training_dynamics": {
      "loss": -1.6523114442825317,
      "current_logprobs": [
        -187.68882751464844,
        -187.71189880371094
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.4762725830078,
        -185.95018005371094
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8123114109039307,
      "kl_div": -181.23114013671875,
      "kl_coef": 0.01,
      "total_loss": -1.6523114442825317,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.568277099886319,
      "clipped_grad_norm": 2.568277359008789,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0033246190287172794
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 97,
    "timing": {
      "total_time": 0.04053624199877959,
      "forward_time": 0.004404491002787836,
      "logprob_time": 0.003923643998859916,
      "loss_time": 0.009431126003619283,
      "backward_time": 0.008818111004075035,
      "optim_time": 0.013958427000034135
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 16951762360904.193,
      "mfu_percent": 10.273795370244967
    },
    "training_dynamics": {
      "loss": -1.664869785308838,
      "current_logprobs": [
        -187.82843017578125,
        -187.8380126953125
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.61587524414062,
        -186.0762939453125
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8248697519302368,
      "kl_div": -182.4869842529297,
      "kl_coef": 0.01,
      "total_loss": -1.664869785308838,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5785635010289587,
      "clipped_grad_norm": 2.578563690185547,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0032125194557011127
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 98,
    "timing": {
      "total_time": 0.04226597999513615,
      "forward_time": 0.004742421995615587,
      "logprob_time": 0.004042877000756562,
      "loss_time": 0.009908426000038162,
      "backward_time": 0.009552132003591396,
      "optim_time": 0.014019360001839232
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 15743829812915.732,
      "mfu_percent": 9.541715038130747
    },
    "training_dynamics": {
      "loss": -1.6619850397109985,
      "current_logprobs": [
        -188.10720825195312,
        -188.09568786621094
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -187.8946533203125,
        -186.33396911621094
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.8219850063323975,
      "kl_div": -182.19850158691406,
      "kl_coef": 0.01,
      "total_loss": -1.6619850397109985,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.5417677070009206,
      "clipped_grad_norm": 2.5417675971984863,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0031165601685643196
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  },
  {
    "epoch": 99,
    "timing": {
      "total_time": 0.04183354300039355,
      "forward_time": 0.005008879998058546,
      "logprob_time": 0.004013131998362951,
      "loss_time": 0.009765620001417119,
      "backward_time": 0.009073246998013929,
      "optim_time": 0.01397171199641889
    },
    "model_analysis": {
      "batch_size": 2,
      "seq_len": 50,
      "tokens_processed": 100,
      "model_params": 124439808,
      "forward_flops": 74663884800,
      "flops_per_sec": 14906303371001.082,
      "mfu_percent": 9.03412325515217
    },
    "training_dynamics": {
      "loss": -1.6688232421875,
      "current_logprobs": [
        -188.3887939453125,
        -188.365478515625
      ],
      "old_logprobs": [
        -0.21254760026931763,
        -1.7617141008377075
      ],
      "logprob_diff": [
        -188.17623901367188,
        -186.603759765625
      ],
      "rewards": [
        1.0,
        0.20000000298023224
      ],
      "policy_loss": 0.1600000113248825,
      "kl_loss": -1.828823208808899,
      "kl_div": -182.88232421875,
      "kl_coef": 0.01,
      "total_loss": -1.6688232421875,
      "baseline": 0.6000000238418579,
      "mean_reward": 0.6000000238418579,
      "std_reward": 0.5656854510307312,
      "mean_advantage": -2.9802322387695312e-08,
      "mean_logprob_ratio": 0.0,
      "fraction_clipped": 1.0
    },
    "gradients": {
      "total_grad_norm": 2.544674715347449,
      "clipped_grad_norm": 2.5446746349334717,
      "param_count": 148,
      "params_with_grad": 148,
      "param_change": 0.0030426818411797285
    },
    "memory": {
      "allocated_gb": 2.7528319358825684,
      "reserved_gb": 3.357421875,
      "peak_gb": 3.218371868133545
    }
  }
]