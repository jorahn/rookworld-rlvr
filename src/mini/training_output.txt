Logging to: logs_continuous_fixed/grpo_training_20250827_184845.log
2025-08-27 18:48:45,215 | grpo_training | INFO | Logging to: logs_continuous_fixed/grpo_training_20250827_184845.log

=== CONFIGURATION ===
2025-08-27 18:48:45,215 | grpo_training | INFO | 
=== CONFIGURATION ===
  model_path: jrahn/RookWorld-LM-124M
2025-08-27 18:48:45,215 | grpo_training | INFO |   model_path: jrahn/RookWorld-LM-124M
  device: cuda
2025-08-27 18:48:45,215 | grpo_training | INFO |   device: cuda
  k_samples: 8
2025-08-27 18:48:45,215 | grpo_training | INFO |   k_samples: 8
  clip_range: 0.2
2025-08-27 18:48:45,215 | grpo_training | INFO |   clip_range: 0.2
  kl_coef: 0.02
2025-08-27 18:48:45,215 | grpo_training | INFO |   kl_coef: 0.02
  baseline_type: group_mean
2025-08-27 18:48:45,215 | grpo_training | INFO |   baseline_type: group_mean
  ema_alpha: 0.1
2025-08-27 18:48:45,215 | grpo_training | INFO |   ema_alpha: 0.1
  baseline_update_freq: 10
2025-08-27 18:48:45,215 | grpo_training | INFO |   baseline_update_freq: 10
  kl_type: forward
2025-08-27 18:48:45,215 | grpo_training | INFO |   kl_type: forward
  adaptive_kl: True
2025-08-27 18:48:45,215 | grpo_training | INFO |   adaptive_kl: True
  kl_target: 0.01
2025-08-27 18:48:45,215 | grpo_training | INFO |   kl_target: 0.01
  kl_horizon: 10000
2025-08-27 18:48:45,215 | grpo_training | INFO |   kl_horizon: 10000
  use_gae: True
2025-08-27 18:48:45,215 | grpo_training | INFO |   use_gae: True
  gae_lambda: 0.95
2025-08-27 18:48:45,215 | grpo_training | INFO |   gae_lambda: 0.95
  value_loss_coef: 0.1
2025-08-27 18:48:45,215 | grpo_training | INFO |   value_loss_coef: 0.1
  entropy_coef: 0.01
2025-08-27 18:48:45,215 | grpo_training | INFO |   entropy_coef: 0.01
  learning_rate: 1e-05
2025-08-27 18:48:45,215 | grpo_training | INFO |   learning_rate: 1e-05
  batch_size: 8
2025-08-27 18:48:45,215 | grpo_training | INFO |   batch_size: 8
  max_steps: 100
2025-08-27 18:48:45,215 | grpo_training | INFO |   max_steps: 100
  grad_clip: 1.0
2025-08-27 18:48:45,215 | grpo_training | INFO |   grad_clip: 1.0
  max_new_tokens: 144
2025-08-27 18:48:45,215 | grpo_training | INFO |   max_new_tokens: 144
  temperature: 0.8
2025-08-27 18:48:45,215 | grpo_training | INFO |   temperature: 0.8
  top_k: 50
2025-08-27 18:48:45,215 | grpo_training | INFO |   top_k: 50
  top_p: 0.95
2025-08-27 18:48:45,215 | grpo_training | INFO |   top_p: 0.95
  n_train_samples: 200
2025-08-27 18:48:45,215 | grpo_training | INFO |   n_train_samples: 200
  n_eval_samples: 50
2025-08-27 18:48:45,215 | grpo_training | INFO |   n_eval_samples: 50
  data_seed: 42
2025-08-27 18:48:45,215 | grpo_training | INFO |   data_seed: 42
  reward_shaping: graduated
2025-08-27 18:48:45,215 | grpo_training | INFO |   reward_shaping: graduated
  continuous_components: {'fen_similarity': 'exponential', 'evaluations': 'linear'}
2025-08-27 18:48:45,215 | grpo_training | INFO |   continuous_components: {'fen_similarity': 'exponential', 'evaluations': 'linear'}
  log_freq: 1
2025-08-27 18:48:45,215 | grpo_training | INFO |   log_freq: 1
  eval_freq: 20
2025-08-27 18:48:45,215 | grpo_training | INFO |   eval_freq: 20
  save_freq: 100
2025-08-27 18:48:45,215 | grpo_training | INFO |   save_freq: 100
  checkpoint_dir: checkpoints/mini_grpo
2025-08-27 18:48:45,215 | grpo_training | INFO |   checkpoint_dir: checkpoints/mini_grpo

=== LOADING MODEL ===
2025-08-27 18:48:45,215 | grpo_training | INFO | 
=== LOADING MODEL ===
Downloading jrahn/RookWorld-LM-124M from HuggingFace...
Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 190650.18it/s]
Loaded model from jrahn/RookWorld-LM-124M
2025-08-27 18:48:46,290 | grpo_training | INFO | Loaded model from jrahn/RookWorld-LM-124M
Model parameters: 124.4M
2025-08-27 18:48:46,291 | grpo_training | INFO | Model parameters: 124.4M
Created reference model for KL regularization
2025-08-27 18:48:46,299 | grpo_training | INFO | Created reference model for KL regularization
Initialized adaptive KL controller (target=0.01)
2025-08-27 18:48:46,299 | grpo_training | INFO | Initialized adaptive KL controller (target=0.01)
Initialized value function
2025-08-27 18:48:46,300 | grpo_training | INFO | Initialized value function

=== LOADING DATA ===
2025-08-27 18:48:46,379 | grpo_training | INFO | 
=== LOADING DATA ===
Loading 200 training samples...
2025-08-27 18:48:46,380 | grpo_training | INFO | Loading 200 training samples...
2025-08-27 18:48:46,380 | dataset | INFO | Loading 200 samples from jrahn/rookworld_7m
2025-08-27 18:48:50,702 | dataset | INFO | Prepared 200 samples - P: 151, A: 49
Loaded 200 training samples
2025-08-27 18:48:50,702 | grpo_training | INFO | Loaded 200 training samples

============================================================
2025-08-27 18:48:51,115 | grpo_training | INFO | 
============================================================
STARTING TRAINING
2025-08-27 18:48:51,115 | grpo_training | INFO | STARTING TRAINING
============================================================
2025-08-27 18:48:51,115 | grpo_training | INFO | ============================================================
2025-08-27 18:48:51,115 | grpo_training | DEBUG | 
Step 1: Sampled 8 prompts
2025-08-27 18:48:51,115 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:48:51,115 | grpo_training | DEBUG | Processing sample 1/8: A task
2025-08-27 18:48:51,557 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r1b2b2/1p3Q2/3k1P1p/p1nN2p1/2P5/5N2/PP3PPP/R1B1KB1R w KQ - 1 18+0.001+0+0...
2025-08-27 18:48:52,962 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:48:53,247 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b7c8 g7g6 f8e7 c7a5 b8a8      E: -7.1 -7.35 -6.89 -7.13 -7.1             B: g7g6...
2025-08-27 18:48:55,192 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:48:55,482 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b7b6 f8b8 a5a2 f8c8 a8b8      E: 2.06 0.79 2.36 1.01 0.6                 B: a8b8...
2025-08-27 18:48:57,498 | grpo_training | DEBUG | Processing sample 4/8: A task
2025-08-27 18:48:57,696 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 1rbqkb1r/2p1pp2/p1Pp1P2/6Bp/2B3p1/2N5/PP2QPPP/R3K1NR b KQk - 1 11+0.001+0+0...
2025-08-27 18:48:59,082 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:48:59,381 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d8d7 d8c7 d8e7                E: -11.95 -11.34 -11.79                    B: d8d7...
2025-08-27 18:49:01,480 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:49:01,849 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f6g5 c7b8 c7h2 f6f7 c7d6      E: -0.42 -0.42 -0.42 -0.41 -0.42           B: c7d6...
2025-08-27 18:49:04,437 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:49:04,721 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c8d7 g7g6 a7a6 c5d4 b8d7      E: -1.06 -0.81 -0.98 -0.78 -1.0            B: c8d7...
2025-08-27 18:49:06,689 | grpo_training | DEBUG | Processing sample 8/8: A task
2025-08-27 18:49:06,842 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 8/2p5/p1P3P1/1p6/5Q2/7p/PPP1RPkP/2K5 b - - 10 42+0.001+0+0...
2025-08-27 18:49:07,923 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:49:07,923 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 1) ===
2025-08-27 18:49:07,923 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:49:07,923 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.508, std=0.381
2025-08-27 18:49:07,923 | grpo_training | DEBUG | Reward distribution: {'1.0': 24, '0.2': 40}
2025-08-27 18:49:07,923 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:49:07,923 | grpo_training | DEBUG | 
[Sample 1] Reward: 1.000
2025-08-27 18:49:07,923 | grpo_training | DEBUG | Completion: r1b2b2/1p3Q2/3k1P1p/p1nN2p1/2P5/5N2/PP3PPP/R1B1KB1R w KQ - 1 18+0.001+0+0...
2025-08-27 18:49:07,923 | grpo_training | DEBUG | 
[Sample 2] Reward: 1.000
2025-08-27 18:49:07,923 | grpo_training | DEBUG | Completion: r1b2b2/1p3Q2/3k1P1p/p1nN2p1/2P5/5N2/PP3PPP/R1B1KB1R w KQ - 1 18+0.001+0+0...
2025-08-27 18:49:07,923 | grpo_training | DEBUG | 
[Sample 3] Reward: 1.000
2025-08-27 18:49:07,923 | grpo_training | DEBUG | Completion: r1b2b2/1p3Q2/3k1P1p/p1nN2p1/2P5/5N2/PP3PPP/R1B1KB1R w KQ - 1 18+0.001+0+0...
2025-08-27 18:49:08,145 | grpo_training | DEBUG | Adaptive KL coefficient: 0.016000

Step 1 | Time: 17.28s
2025-08-27 18:49:08,404 | grpo_training | INFO | 
Step 1 | Time: 17.28s
  Loss: -0.0421
2025-08-27 18:49:08,404 | grpo_training | INFO |   Loss: -0.0421
  PG Loss: -0.0000
2025-08-27 18:49:08,404 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: 0.0000
2025-08-27 18:49:08,404 | grpo_training | INFO |   KL Divergence: 0.0000
2025-08-27 18:49:08,404 | grpo_training | DEBUG |   KL Forward: 0.0000
2025-08-27 18:49:08,404 | grpo_training | DEBUG |   KL Reverse: 0.0000
2025-08-27 18:49:08,404 | grpo_training | DEBUG |   KL Symmetric: 0.0000
2025-08-27 18:49:08,404 | grpo_training | DEBUG |   Ratio Outliers: 0.000%
2025-08-27 18:49:08,404 | grpo_training | DEBUG |   Gradient Norm: 0.5388
2025-08-27 18:49:08,504 | grpo_training | DEBUG | 
Step 2: Sampled 8 prompts
2025-08-27 18:49:08,504 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:49:08,504 | grpo_training | DEBUG | Processing sample 1/8: A task
2025-08-27 18:49:08,663 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 8/1P6/k1RB4/4p2p/1q6/p4P2/P3R1KP/8 b - - 1 55+0.001+0+0...
2025-08-27 18:49:09,714 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:49:10,011 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h5g6 f3g5 a1d1 e1c1 c3b3      E: 8.68 8.35 7.96 7.91 7.75                B: h5g6...
2025-08-27 18:49:12,091 | grpo_training | DEBUG | Processing sample 3/8: A task
2025-08-27 18:49:12,223 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 8/8/8/3p2N1/3Q3P/p2n4/R5PK/8 b - - 4 67+0.001+0+0...
2025-08-27 18:49:13,138 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:49:13,454 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e7e6 e7d7 e7f7 d6e8 e7f6      E: -3.14 -3.15 -3.01 -3.19 -3.13           B: d6e8...
2025-08-27 18:49:15,677 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:49:15,954 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a6a5 f8e7 b7c8 g7g5 g7g6      E: -7.05 -7.12 -7.03 -6.99 -7.09           B: f8e7...
2025-08-27 18:49:17,912 | grpo_training | DEBUG | Processing sample 6/8: A task
2025-08-27 18:49:18,091 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 4k3/ppB3p1/1Nn1p3/P3P2p/3p4/P7/2P2PPP/3R1RK1 w - - 2 31+0.001+0+0...
2025-08-27 18:49:19,345 | grpo_training | DEBUG | Processing sample 7/8: A task
2025-08-27 18:49:19,519 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: Qnb1k1nr/p4ppp/2qbp3/8/2PP4/5N2/PP3PPP/RNBQKB1R b KQk - 2 8+0.001+0+0...
2025-08-27 18:49:20,725 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:49:21,049 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a6a5 a6a7 h2h3 a4a5 g2g3      E: 0.15 0.12 0.3 0.28 0.2                  B: h2h3...
2025-08-27 18:49:23,296 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:49:23,296 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 2) ===
2025-08-27 18:49:23,296 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:49:23,296 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.606, std=0.394
2025-08-27 18:49:23,296 | grpo_training | DEBUG | Reward distribution: {'1.0': 32, '0.2': 32}
2025-08-27 18:49:23,296 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:49:23,296 | grpo_training | DEBUG | 
[Sample 1] Reward: 1.000
2025-08-27 18:49:23,296 | grpo_training | DEBUG | Completion: 8/1P6/k1RB4/4p2p/1q6/p4P2/P3R1KP/8 b - - 1 55+0.001+0+0...
2025-08-27 18:49:23,296 | grpo_training | DEBUG | 
[Sample 2] Reward: 1.000
2025-08-27 18:49:23,296 | grpo_training | DEBUG | Completion: 8/1P6/k1RB4/4p2p/1q6/p4P2/P3R1KP/8 b - - 1 55+0.001+0+0...
2025-08-27 18:49:23,296 | grpo_training | DEBUG | 
[Sample 3] Reward: 1.000
2025-08-27 18:49:23,296 | grpo_training | DEBUG | Completion: 8/1P6/k1RB4/4p2p/1q6/p4P2/P3R1KP/8 b - - 1 55+0.001+0+0...
2025-08-27 18:49:23,490 | grpo_training | DEBUG | Adaptive KL coefficient: 0.012800

Step 2 | Time: 15.16s
2025-08-27 18:49:23,671 | grpo_training | INFO | 
Step 2 | Time: 15.16s
  Loss: -0.0336
2025-08-27 18:49:23,671 | grpo_training | INFO |   Loss: -0.0336
  PG Loss: -0.0000
2025-08-27 18:49:23,671 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.0331
2025-08-27 18:49:23,671 | grpo_training | INFO |   KL Divergence: -0.0331
2025-08-27 18:49:23,671 | grpo_training | DEBUG |   KL Forward: -0.0331
2025-08-27 18:49:23,671 | grpo_training | DEBUG |   KL Reverse: 0.0405
2025-08-27 18:49:23,671 | grpo_training | DEBUG |   KL Symmetric: 0.0037
2025-08-27 18:49:23,671 | grpo_training | DEBUG |   Ratio Outliers: 0.000%
2025-08-27 18:49:23,671 | grpo_training | DEBUG |   Gradient Norm: 0.4027
2025-08-27 18:49:23,771 | grpo_training | DEBUG | 
Step 3: Sampled 8 prompts
2025-08-27 18:49:23,772 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:49:23,772 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:49:24,079 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a2a4 f1d3 c2c3 b1c3 g1f3      E: 0.71 0.81 0.79 0.78 0.91                B: g1f3...
2025-08-27 18:49:26,211 | grpo_training | DEBUG | Processing sample 2/8: A task
2025-08-27 18:49:26,409 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r1bqkbnr/1ppnpp1p/3p2p1/p7/3PPP2/2N5/PPP3PP/R1BQKBNR w KQkq - 0 5+0.001+0+0...
2025-08-27 18:49:27,799 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:49:28,073 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c6a7 f8e8 b7b5 a8c8 h5g6      E: 0.13 0.21 0.19 0.2 0.29                 B: c6a7...
2025-08-27 18:49:29,988 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:49:30,272 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g4f3 e8c8 d5d6 d5a5 b4c3      E: 0.13 0.27 -0.07 0.11 0.07               B: d5d6...
2025-08-27 18:49:32,267 | grpo_training | DEBUG | Processing sample 5/8: A task
2025-08-27 18:49:32,421 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 3N4/8/8/pQ6/1p1BP1pp/1P6/P4PPP/R3K2R b KQ - 1 39+0.001+0+0...
2025-08-27 18:49:33,493 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:49:33,779 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b5d6 d1e1 d1e2 f1e1 e5d4      E: -3.56 -4.0 -3.78 -3.89 -4.21            B: b5d6...
2025-08-27 18:49:35,777 | grpo_training | DEBUG | Processing sample 7/8: A task
2025-08-27 18:49:35,972 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 5r1r/1b1p1pp1/n3pn2/BN2N3/p1p1P3/P6P/1PP1BPP1/R2R2K1 b - - 0 23+0.001+0+0...
2025-08-27 18:49:37,336 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:49:37,622 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b6d5 h7h6 b4d5 d8d1 e8g8      E: -1.14 -1.17 -1.04 -0.99 -1.12           B: h7h6...
2025-08-27 18:49:39,636 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:49:39,636 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 3) ===
2025-08-27 18:49:39,636 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:49:39,636 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.508, std=0.381
2025-08-27 18:49:39,636 | grpo_training | DEBUG | Reward distribution: {'0.2': 40, '1.0': 24}
2025-08-27 18:49:39,636 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:49:39,636 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:49:39,636 | grpo_training | DEBUG | Completion: M: a2a4 f1d3 c2c3 b1c3 g1f3      E: 0.71 0.81 0.79 0.78 0.91                B: g1f3...
2025-08-27 18:49:39,636 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:49:39,636 | grpo_training | DEBUG | Completion: M: c2c3 g1f3 f1d3 h2h3 b1c3      E: 0.84 0.97 0.92 0.87 0.84                B: g1f3...
2025-08-27 18:49:39,636 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:49:39,636 | grpo_training | DEBUG | Completion: M: c2c3 g1f3 b1c3 f1d3 h2h4      E: 0.91 0.96 0.86 0.91 0.88                B: c2c3...
2025-08-27 18:49:39,847 | grpo_training | DEBUG | Adaptive KL coefficient: 0.010240

Step 3 | Time: 16.25s
2025-08-27 18:49:40,033 | grpo_training | INFO | 
Step 3 | Time: 16.25s
  Loss: -0.0363
2025-08-27 18:49:40,033 | grpo_training | INFO |   Loss: -0.0363
  PG Loss: -0.0000
2025-08-27 18:49:40,033 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.0325
2025-08-27 18:49:40,033 | grpo_training | INFO |   KL Divergence: -0.0325
2025-08-27 18:49:40,033 | grpo_training | DEBUG |   KL Forward: -0.0325
2025-08-27 18:49:40,033 | grpo_training | DEBUG |   KL Reverse: 0.0349
2025-08-27 18:49:40,033 | grpo_training | DEBUG |   KL Symmetric: 0.0012
2025-08-27 18:49:40,033 | grpo_training | DEBUG |   Ratio Outliers: 0.000%
2025-08-27 18:49:40,033 | grpo_training | DEBUG |   Gradient Norm: 0.3393
2025-08-27 18:49:40,140 | grpo_training | DEBUG | 
Step 4: Sampled 8 prompts
2025-08-27 18:49:40,140 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:49:40,140 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:49:40,516 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c7h2 c7b8 f6g5 c7e5 f6f7      E: -7.28 -7.27 -7.34 -7.27 -7.36           B: f6f7...
2025-08-27 18:49:43,112 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:49:43,438 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a2c4 b2d2 d3b4 a2a6 a2b3      E: 4.59 3.97 3.96 4.45 4.59                B: b2d2...
2025-08-27 18:49:45,722 | grpo_training | DEBUG | Processing sample 3/8: A task
2025-08-27 18:49:45,917 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 5r1r/1b1p1pp1/n3pn2/BN2N3/p1p1P3/P6P/1PP1BPP1/R2R2K1 b - - 0 23+0.001+0+0...
2025-08-27 18:49:47,280 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:49:47,574 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e5d4 f6e8 c8f5 c6d4 c8d7      E: 1.33 0.67 1.11 1.32 1.44                B: f6e8...
2025-08-27 18:49:49,625 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:49:49,916 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f1g3 e1e6 f1e3 c1e3 e1e3      E: -2.41 -1.96 -1.91 -2.36 -4.09           B: f1e3...
2025-08-27 18:49:51,954 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:49:52,256 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e5f6 a4b3 e5d6 a4a3 e5c7      E: -1.01 -0.97 0.02 -0.85 -0.91            B: e5f6...
2025-08-27 18:49:54,382 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:49:54,662 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8g7 f6h7 c7d8 e5e4 c7e7      E: -0.79 -2.08 -1.6 -2.33 -1.8             B: e5e4...
2025-08-27 18:49:56,594 | grpo_training | DEBUG | Processing sample 8/8: A task
2025-08-27 18:49:56,774 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 4k3/ppB3p1/1Nn1p3/P3P2p/3p4/P7/2P2PPP/3R1RK1 w - - 2 31+0.001+0+0...
2025-08-27 18:49:58,026 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:49:58,026 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 4) ===
2025-08-27 18:49:58,026 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:49:58,026 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.409, std=0.341
2025-08-27 18:49:58,026 | grpo_training | DEBUG | Reward distribution: {'0.2': 48, '1.0': 16}
2025-08-27 18:49:58,026 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:49:58,026 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:49:58,026 | grpo_training | DEBUG | Completion: M: c7h2 c7b8 f6g5 c7e5 f6f7      E: -7.28 -7.27 -7.34 -7.27 -7.36           B: f6f7...
2025-08-27 18:49:58,026 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:49:58,026 | grpo_training | DEBUG | Completion: M: f6g5 c7d6 c7b8 c7h2 f6e7      E: -4.79 -4.72 -4.64 -4.73 -4.64           B: f6g5...
2025-08-27 18:49:58,026 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:49:58,026 | grpo_training | DEBUG | Completion: M: f6g5 f6e7 c7b8 c7h2 c7d6      E: -7.38 -6.62 -6.58 -6.58 -6.68           B: c7d6...
2025-08-27 18:49:58,238 | grpo_training | DEBUG | Adaptive KL coefficient: 0.008192

Step 4 | Time: 18.27s
2025-08-27 18:49:58,425 | grpo_training | INFO | 
Step 4 | Time: 18.27s
  Loss: -0.0572
2025-08-27 18:49:58,425 | grpo_training | INFO |   Loss: -0.0572
  PG Loss: -0.0000
2025-08-27 18:49:58,425 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.0518
2025-08-27 18:49:58,425 | grpo_training | INFO |   KL Divergence: -0.0518
2025-08-27 18:49:58,425 | grpo_training | DEBUG |   KL Forward: -0.0518
2025-08-27 18:49:58,425 | grpo_training | DEBUG |   KL Reverse: 0.1228
2025-08-27 18:49:58,425 | grpo_training | DEBUG |   KL Symmetric: 0.0355
2025-08-27 18:49:58,425 | grpo_training | DEBUG |   Ratio Outliers: 10.938%
2025-08-27 18:49:58,425 | grpo_training | DEBUG |   Gradient Norm: 0.4071
2025-08-27 18:49:58,528 | grpo_training | DEBUG | 
Step 5: Sampled 8 prompts
2025-08-27 18:49:58,528 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:49:58,528 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:49:58,923 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h3g2 f7h7 f7a7 f7e7 f7f8      E: 0.0 0.0 0.0 0.0 0.0                     B: f7f8...
2025-08-27 18:50:01,631 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:50:01,920 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f2f4 d4d5 g2g4 g1f3 e4e5      E: 1.25 1.36 1.34 1.21 1.43                B: e4e5...
2025-08-27 18:50:03,949 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:50:04,238 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h8e5 d1b1 f3g5 c2a2 h8h7      E: 11.57 12.22 999.96 10.99 13.14          B: f3g5...
2025-08-27 18:50:06,178 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:50:06,507 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c6d7 c6e8 c6d5 a8a6 d6d5      E: -3.45 -3.65 -2.89 -3.93 -2.89           B: a8a6...
2025-08-27 18:50:08,815 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:50:09,093 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d6e7 d6c7 d5e4 d6b4 f6e8      E: -0.45 -0.2 -0.32 -0.45 -0.29            B: d6e7...
2025-08-27 18:50:11,014 | grpo_training | DEBUG | Processing sample 6/8: A task
2025-08-27 18:50:11,202 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: rnbqk1nr/p2pbppp/1pp1p3/8/3PP1Q1/2PB4/PP3PPP/RNB1K1NR b KQkq - 1 5+0.001+0+0...
2025-08-27 18:50:12,516 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:50:12,837 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e4g4 g7g5 e4f5 f8c8 h7h6      E: 2.56 2.37 2.44 2.56 2.35                B: h7h6...
2025-08-27 18:50:15,095 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:50:15,457 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f6g6 f6e5                     E: -999.99 -3.26                           B: f6e5...
2025-08-27 18:50:18,018 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:50:18,018 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 5) ===
2025-08-27 18:50:18,018 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:50:18,018 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.311, std=0.260
2025-08-27 18:50:18,019 | grpo_training | DEBUG | Reward distribution: {'0.2': 56, '1.0': 8}
2025-08-27 18:50:18,019 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:50:18,019 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:50:18,019 | grpo_training | DEBUG | Completion: M: h3g2 f7h7 f7a7 f7e7 f7f8      E: 0.0 0.0 0.0 0.0 0.0                     B: f7f8...
2025-08-27 18:50:18,019 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:50:18,019 | grpo_training | DEBUG | Completion: M: f7e7 f7f8 h3g2 h3h2 f7f6      E: 0.0 0.0 0.0 0.0 0.0                     B: f7f8...
2025-08-27 18:50:18,019 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:50:18,026 | grpo_training | DEBUG | Completion: M: f7e7 f7h7 f7a7 h3g2 f7g7      E: 0.0 0.0 0.0 0.0 0.0                     B: f7e7...
2025-08-27 18:50:18,232 | grpo_training | DEBUG | Adaptive KL coefficient: 0.006554

Step 5 | Time: 19.88s
2025-08-27 18:50:18,415 | grpo_training | INFO | 
Step 5 | Time: 19.88s
  Loss: -0.0487
2025-08-27 18:50:18,415 | grpo_training | INFO |   Loss: -0.0487
  PG Loss: -0.0000
2025-08-27 18:50:18,415 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.1035
2025-08-27 18:50:18,415 | grpo_training | INFO |   KL Divergence: -0.1035
2025-08-27 18:50:18,415 | grpo_training | DEBUG |   KL Forward: -0.1035
2025-08-27 18:50:18,415 | grpo_training | DEBUG |   KL Reverse: 0.1175
2025-08-27 18:50:18,415 | grpo_training | DEBUG |   KL Symmetric: 0.0070
2025-08-27 18:50:18,415 | grpo_training | DEBUG |   Ratio Outliers: 4.688%
2025-08-27 18:50:18,415 | grpo_training | DEBUG |   Gradient Norm: 0.3594
2025-08-27 18:50:18,516 | grpo_training | DEBUG | 
Step 6: Sampled 8 prompts
2025-08-27 18:50:18,516 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:50:18,516 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:50:18,897 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f4g3 f4e3 f4f3                E: -5.89 -5.47 -5.77                       B: f4e3...
2025-08-27 18:50:21,527 | grpo_training | DEBUG | Processing sample 2/8: A task
2025-08-27 18:50:21,732 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r2Q2nr/1p1kp2p/2p2Ppb/p5B1/3P2b1/2N2N2/PP2BPPP/2RQ1RK1 b - - 2 17+0.001+0+0...
2025-08-27 18:50:23,159 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:50:23,483 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a1c1 g1h1 g2g4 h4h5 c3f6      E: -6.37 -6.32 -6.33 -6.44 -6.41           B: g1h1...
2025-08-27 18:50:25,773 | grpo_training | DEBUG | Processing sample 4/8: A task
2025-08-27 18:50:25,991 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r1bqkbnr/1p1ppp2/n1p3pp/p3P3/3P1B2/P1NB1N2/1PP2PPP/R2QK2R b KQkq - 2 8+0.001+0+0...
2025-08-27 18:50:27,508 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:50:27,810 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b6b5 f8f5 b6c7 b6c6 c8d7      E: 0.01 -0.2 -0.07 0.07 0.04               B: f8f5...
2025-08-27 18:50:29,904 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:50:30,232 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a2b3 a2a4 b2b3 d3c5 e8d8      E: 4.46 4.2 4.23 4.21 3.94                 B: e8d8...
2025-08-27 18:50:32,509 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:50:32,800 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b8c8 e6g4 f6h5 f8e8 a7a6      E: -1.1 -1.14 -0.97 -1.09 -0.96            B: e6g4...
2025-08-27 18:50:34,826 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:50:35,179 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e1e3 g2h3 g2f3 a2a4 b3b4      E: 0.01 -4.81 -4.82 -4.9 -4.72             B: e1e3...
2025-08-27 18:50:37,642 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:50:37,642 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 6) ===
2025-08-27 18:50:37,643 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:50:37,643 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.409, std=0.341
2025-08-27 18:50:37,643 | grpo_training | DEBUG | Reward distribution: {'0.2': 48, '1.0': 16}
2025-08-27 18:50:37,643 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:50:37,643 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:50:37,643 | grpo_training | DEBUG | Completion: M: f4g3 f4e3 f4f3                E: -5.89 -5.47 -5.77                       B: f4e3...
2025-08-27 18:50:37,643 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:50:37,643 | grpo_training | DEBUG | Completion: M: f4g3 f4e3 f4f3                E: -6.45 -6.22 -6.26                       B: f4e3...
2025-08-27 18:50:37,643 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:50:37,643 | grpo_training | DEBUG | Completion: M: f4g3 f4e3 f4f3                E: -7.01 -7.01 -7.16                       B: f4e3...
2025-08-27 18:50:37,873 | grpo_training | DEBUG | Adaptive KL coefficient: 0.005243

Step 6 | Time: 19.55s
2025-08-27 18:50:38,078 | grpo_training | INFO | 
Step 6 | Time: 19.55s
  Loss: -0.0663
2025-08-27 18:50:38,078 | grpo_training | INFO |   Loss: -0.0663
  PG Loss: -0.0000
2025-08-27 18:50:38,078 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.1765
2025-08-27 18:50:38,078 | grpo_training | INFO |   KL Divergence: -0.1765
2025-08-27 18:50:38,079 | grpo_training | DEBUG |   KL Forward: -0.1765
2025-08-27 18:50:38,079 | grpo_training | DEBUG |   KL Reverse: 0.2091
2025-08-27 18:50:38,079 | grpo_training | DEBUG |   KL Symmetric: 0.0163
2025-08-27 18:50:38,079 | grpo_training | DEBUG |   Ratio Outliers: 9.375%
2025-08-27 18:50:38,079 | grpo_training | DEBUG |   Gradient Norm: 0.2972
2025-08-27 18:50:38,191 | grpo_training | DEBUG | 
Step 7: Sampled 8 prompts
2025-08-27 18:50:38,191 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:50:38,191 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:50:38,483 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e8c8 d5h5 f6e4 g4f3 d5a5      E: 0.36 0.14 0.31 0.19 0.16                B: g4f3...
2025-08-27 18:50:40,481 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:50:40,846 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f6e5 f6g6                     E: 0.0 -999.98                             B: f6e5...
2025-08-27 18:50:43,445 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:50:43,745 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d1a4 a2a4 c1e3 c1g5 d1d8      E: 0.04 0.05 0.2 0.03 0.0                  B: c1e3...
2025-08-27 18:50:45,809 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:50:46,140 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b1f1 f3g3 b1c1 b1a1 b1d1      E: 0.0 0.0 0.0 0.0 0.0                     B: b1d1...
2025-08-27 18:50:48,421 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:50:48,773 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g6c2 g6d3 g6b1 g6e8 c7b8      E: -10.01 -10.01 -10.01 -10.01 -9.85       B: g6b1...
2025-08-27 18:50:51,302 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:50:51,593 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b1d2 f1e2 c2a4 f2f4 c1f4      E: 0.04 -0.33 -0.41 -0.15 -0.35            B: b1d2...
2025-08-27 18:50:53,657 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:50:54,006 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8h8 g8f8                     E: 1.06 0.81                               B: g8f8...
2025-08-27 18:50:56,457 | grpo_training | DEBUG | Processing sample 8/8: A task
2025-08-27 18:50:56,607 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 8/1P6/k1RB4/4p2p/1q6/p4P2/P3R1KP/8 b - - 1 55+0.001+0+0...
2025-08-27 18:50:57,655 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:50:57,655 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 7) ===
2025-08-27 18:50:57,655 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:50:57,655 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.311, std=0.260
2025-08-27 18:50:57,656 | grpo_training | DEBUG | Reward distribution: {'0.2': 56, '1.0': 8}
2025-08-27 18:50:57,656 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:50:57,656 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:50:57,656 | grpo_training | DEBUG | Completion: M: e8c8 d5h5 f6e4 g4f3 d5a5      E: 0.36 0.14 0.31 0.19 0.16                B: g4f3...
2025-08-27 18:50:57,656 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:50:57,656 | grpo_training | DEBUG | Completion: M: b4c3 d5a5 a8d8 e8c8 e8g8      E: -0.2 -0.16 -0.15 0.15 -0.02             B: b4c3...
2025-08-27 18:50:57,656 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:50:57,656 | grpo_training | DEBUG | Completion: M: d5a5 d5h5 d5d7 e8c8 b4c3      E: -0.03 -0.28 -0.13 0.25 -0.23            B: d5h5...
2025-08-27 18:50:57,868 | grpo_training | DEBUG | Adaptive KL coefficient: 0.004194

Step 7 | Time: 19.85s
2025-08-27 18:50:58,053 | grpo_training | INFO | 
Step 7 | Time: 19.85s
  Loss: -0.0460
2025-08-27 18:50:58,053 | grpo_training | INFO |   Loss: -0.0460
  PG Loss: -0.0000
2025-08-27 18:50:58,053 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.3208
2025-08-27 18:50:58,053 | grpo_training | INFO |   KL Divergence: -0.3208
2025-08-27 18:50:58,053 | grpo_training | DEBUG |   KL Forward: -0.3208
2025-08-27 18:50:58,053 | grpo_training | DEBUG |   KL Reverse: 0.4745
2025-08-27 18:50:58,053 | grpo_training | DEBUG |   KL Symmetric: 0.0768
2025-08-27 18:50:58,053 | grpo_training | DEBUG |   Ratio Outliers: 26.562%
2025-08-27 18:50:58,053 | grpo_training | DEBUG |   Gradient Norm: 0.2520
2025-08-27 18:50:58,156 | grpo_training | DEBUG | 
Step 8: Sampled 8 prompts
2025-08-27 18:50:58,157 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:50:58,157 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:50:58,458 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d2e1 d2b4 d2f2 d2a5 c8c2      E: 0.77 1.05 0.64 0.6 0.26                 B: c8c2...
2025-08-27 18:51:00,517 | grpo_training | DEBUG | Processing sample 2/8: A task
2025-08-27 18:51:00,696 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 2b1kb2/2p2p1p/p1pppQ2/8/4N3/8/PPP2PPP/R1BQKBNR w KQ - 0 12+0.001+0+0...
2025-08-27 18:51:01,948 | grpo_training | DEBUG | Processing sample 3/8: A task
2025-08-27 18:51:02,101 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 8/2p5/p1P3P1/1p6/5Q2/7p/PPP1RPkP/2K5 b - - 10 42+0.001+0+0...
2025-08-27 18:51:03,171 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:51:03,477 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f2h3 g3g6 f2d1 f7f5 g3h3      E: -1.71 -2.93 -4.24 -4.32 -3.7            B: f7f5...
2025-08-27 18:51:05,612 | grpo_training | DEBUG | Processing sample 5/8: A task
2025-08-27 18:51:05,768 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 5k2/8/2p5/2p4p/R1BP1BN1/5N2/1P3PPP/6K1 w - - 0 37+0.001+0+0...
2025-08-27 18:51:06,860 | grpo_training | DEBUG | Processing sample 6/8: A task
2025-08-27 18:51:07,065 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r1b1k2r/2P1bpp1/pp1P2n1/nB5p/5pP1/P4N2/1PPK3P/R1BQR3 b kq - 3 20+0.001+0+0...
2025-08-27 18:51:08,494 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:51:08,814 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d6f7 h7h6 e7f7 e7f6 g6g5      E: -2.92 -2.72 -2.8 -2.7 -2.55             B: d6f7...
2025-08-27 18:51:11,030 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:51:11,338 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d4d3 a6c5 g7g6 c8b8 c6b5      E: -3.6 -3.37 -3.56 -3.44 -3.44            B: d4d3...
2025-08-27 18:51:13,481 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:51:13,482 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 8) ===
2025-08-27 18:51:13,482 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:51:13,482 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.606, std=0.394
2025-08-27 18:51:13,482 | grpo_training | DEBUG | Reward distribution: {'0.2': 32, '1.0': 32}
2025-08-27 18:51:13,482 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:51:13,482 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:51:13,482 | grpo_training | DEBUG | Completion: M: d2e1 d2b4 d2f2 d2a5 c8c2      E: 0.77 1.05 0.64 0.6 0.26                 B: c8c2...
2025-08-27 18:51:13,482 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:51:13,482 | grpo_training | DEBUG | Completion: M: d2f2 d2e1 d2b4 d2a5 f7f5      E: 0.82 2.45 0.91 0.77 0.35                B: f7f5...
2025-08-27 18:51:13,482 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:51:13,482 | grpo_training | DEBUG | Completion: M: d2c1 d2b4 d2e1 d2d3 d2c3      E: -4.56 2.45 2.45 -3.64 -3.44             B: d2c1...
2025-08-27 18:51:13,697 | grpo_training | DEBUG | Adaptive KL coefficient: 0.003355

Step 8 | Time: 15.72s
2025-08-27 18:51:13,887 | grpo_training | INFO | 
Step 8 | Time: 15.72s
  Loss: -0.0494
2025-08-27 18:51:13,887 | grpo_training | INFO |   Loss: -0.0494
  PG Loss: -0.0000
2025-08-27 18:51:13,887 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.2064
2025-08-27 18:51:13,887 | grpo_training | INFO |   KL Divergence: -0.2064
2025-08-27 18:51:13,887 | grpo_training | DEBUG |   KL Forward: -0.2064
2025-08-27 18:51:13,887 | grpo_training | DEBUG |   KL Reverse: 0.4198
2025-08-27 18:51:13,887 | grpo_training | DEBUG |   KL Symmetric: 0.1067
2025-08-27 18:51:13,887 | grpo_training | DEBUG |   Ratio Outliers: 26.562%
2025-08-27 18:51:13,887 | grpo_training | DEBUG |   Gradient Norm: 0.2532
2025-08-27 18:51:13,998 | grpo_training | DEBUG | 
Step 9: Sampled 8 prompts
2025-08-27 18:51:13,998 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:51:13,998 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:51:14,335 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g1g2 a2a4 d4d8 h3f4 d4d6      E: 1.06 0.78 0.97 1.4 1.15                 B: h3f4...
2025-08-27 18:51:16,638 | grpo_training | DEBUG | Processing sample 2/8: A task
2025-08-27 18:51:16,803 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r7/3QN3/p1N5/5p1p/3P3p/1Q6/PP2PPPP/R3KB1R b KQ - 1 26+0.001+0+0...
2025-08-27 18:51:17,962 | grpo_training | DEBUG | Processing sample 3/8: A task
2025-08-27 18:51:18,172 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: rn2k1nr/pb1qbp2/1pp1p1p1/3pP2p/3P4/2PBBN1P/PP1N1PP1/R2Q1RK1 b kq - 3 10+0.001+0+0...
2025-08-27 18:51:19,643 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:51:19,948 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g3f4 d8d4 g3g6 g3h2 f7f6      E: -3.72 -2.94 -2.73 -2.81 -3.76           B: f7f6...
2025-08-27 18:51:22,094 | grpo_training | DEBUG | Processing sample 5/8: A task
2025-08-27 18:51:22,298 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r2Q2nr/1p1kp2p/2p2Ppb/p5B1/3P2b1/2N2N2/PP2BPPP/2RQ1RK1 b - - 2 17+0.001+0+0...
2025-08-27 18:51:23,726 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:51:24,014 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a4c6 a4a5 a4d4 a4e8 a4a2      E: -1.17 -4.07 -4.64 -3.01 -5.02           B: a4a2...
2025-08-27 18:51:26,052 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:51:26,354 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d1b1 h2h1 g2f1 d1e1 c2b2      E: -0.88 -0.86 -0.79 -0.9 -1.02            B: g2f1...
2025-08-27 18:51:28,435 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:51:28,826 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d8d7 d8f8 d8d5 a7b7 d8d4      E: -0.03 -0.02 -0.03 -0.03 -0.03           B: a7b7...
2025-08-27 18:51:31,569 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:51:31,569 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 9) ===
2025-08-27 18:51:31,569 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:51:31,569 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.508, std=0.381
2025-08-27 18:51:31,569 | grpo_training | DEBUG | Reward distribution: {'0.2': 40, '1.0': 24}
2025-08-27 18:51:31,569 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:51:31,569 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:51:31,569 | grpo_training | DEBUG | Completion: M: g1g2 a2a4 d4d8 h3f4 d4d6      E: 1.06 0.78 0.97 1.4 1.15                 B: h3f4...
2025-08-27 18:51:31,569 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:51:31,569 | grpo_training | DEBUG | Completion: M: a2a3 d4d8 d1d2 d4d6 h3f4      E: 0.97 1.23 1.3 1.35 1.45                 B: h3f4...
2025-08-27 18:51:31,569 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:51:31,569 | grpo_training | DEBUG | Completion: M: d4d8 d4d2 g1g2 d1d2 h3f4      E: 1.01 1.27 1.31 1.3 1.26                 B: g1g2...
2025-08-27 18:51:31,793 | grpo_training | DEBUG | Adaptive KL coefficient: 0.002684

Step 9 | Time: 17.98s
2025-08-27 18:51:31,991 | grpo_training | INFO | 
Step 9 | Time: 17.98s
  Loss: -0.0620
2025-08-27 18:51:31,991 | grpo_training | INFO |   Loss: -0.0620
  PG Loss: -0.0000
2025-08-27 18:51:31,991 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.2635
2025-08-27 18:51:31,991 | grpo_training | INFO |   KL Divergence: -0.2635
2025-08-27 18:51:31,991 | grpo_training | DEBUG |   KL Forward: -0.2635
2025-08-27 18:51:31,991 | grpo_training | DEBUG |   KL Reverse: 0.2998
2025-08-27 18:51:31,991 | grpo_training | DEBUG |   KL Symmetric: 0.0182
2025-08-27 18:51:31,991 | grpo_training | DEBUG |   Ratio Outliers: 10.938%
2025-08-27 18:51:31,991 | grpo_training | DEBUG |   Gradient Norm: 0.2873
2025-08-27 18:51:32,103 | grpo_training | DEBUG | 
Step 10: Sampled 8 prompts
2025-08-27 18:51:32,103 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:51:32,103 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:51:32,425 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f1g1 f1e1 f1f2                E: -7.04 -5.75 -5.12                       B: f1f2...
2025-08-27 18:51:34,622 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:51:34,918 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c3b3 f3g5 f3d2 h5g6 c3e3      E: 7.77 8.44 8.29 8.65 8.14                B: h5g6...
2025-08-27 18:51:37,006 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:51:37,338 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f8g8 f8f1 g5g4 f8f4 h5h4      E: -2.26 -2.01 -2.26 -2.36 -2.04           B: f8f4...
2025-08-27 18:51:39,686 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:51:39,986 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f5d4 c8d7 d8d6 d8f6 h7h5      E: 0.12 -4.39 -2.84 -3.75 -4.48            B: h7h5...
2025-08-27 18:51:42,098 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:51:42,401 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h6h5 a7a6 h8b8 g5g4 f7f5      E: 3.08 2.88 2.72 2.97 3.08                B: a7a6...
2025-08-27 18:51:44,554 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:51:44,865 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f4g3 e4e3 g8e8 g8f8 g8h8      E: -0.01 0.0 -0.22 0.0 -1.04               B: g8h8...
2025-08-27 18:51:47,009 | grpo_training | DEBUG | Processing sample 7/8: A task
2025-08-27 18:51:47,226 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r1bqkbnr/1p1ppp2/n1p3pp/p3P3/3P1B2/P1NB1N2/1PP2PPP/R2QK2R b KQkq - 2 8+0.001+0+0...
2025-08-27 18:51:48,743 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:51:49,061 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a2a3 a2a7 a2e2 e4d2 a2a4      E: -1.33 -1.33 -1.15 -0.84 -1.35           B: a2a7...
2025-08-27 18:51:51,310 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=1.000
2025-08-27 18:51:51,310 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 10) ===
2025-08-27 18:51:51,310 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:51:51,310 | grpo_training | DEBUG | Rewards: min=0.157, max=1.000, mean=0.310, std=0.261
2025-08-27 18:51:51,310 | grpo_training | DEBUG | Reward distribution: {'0.2': 56, '1.0': 8}
2025-08-27 18:51:51,310 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:51:51,310 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:51:51,310 | grpo_training | DEBUG | Completion: M: f1g1 f1e1 f1f2                E: -7.04 -5.75 -5.12                       B: f1f2...
2025-08-27 18:51:51,310 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:51:51,310 | grpo_training | DEBUG | Completion: M: f1f2 f1e1 f1g1                E: -0.25 -7.12 -7.21                       B: f1f2...
2025-08-27 18:51:51,310 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.157
2025-08-27 18:51:51,310 | grpo_training | DEBUG | Completion: M: c2d1 c4d5 b1b7 b1d1 f1f2      E: -6.26 -7.18 -7.21 -5.99 -2.19           B: f1f2...
2025-08-27 18:51:51,540 | grpo_training | DEBUG | Adaptive KL coefficient: 0.002147

Step 10 | Time: 19.63s
2025-08-27 18:51:51,743 | grpo_training | INFO | 
Step 10 | Time: 19.63s
  Loss: 0.2628
2025-08-27 18:51:51,743 | grpo_training | INFO |   Loss: 0.2628
  PG Loss: 0.3482
2025-08-27 18:51:51,743 | grpo_training | INFO |   PG Loss: 0.3482
  KL Divergence: -0.4394
2025-08-27 18:51:51,743 | grpo_training | INFO |   KL Divergence: -0.4394
2025-08-27 18:51:51,743 | grpo_training | DEBUG |   KL Forward: -0.4394
2025-08-27 18:51:51,743 | grpo_training | DEBUG |   KL Reverse: 0.5579
2025-08-27 18:51:51,743 | grpo_training | DEBUG |   KL Symmetric: 0.0592
2025-08-27 18:51:51,743 | grpo_training | DEBUG |   Ratio Outliers: 32.812%
2025-08-27 18:51:51,743 | grpo_training | DEBUG |   Gradient Norm: 444.0707
2025-08-27 18:51:51,849 | grpo_training | DEBUG | 
Step 11: Sampled 8 prompts
2025-08-27 18:51:51,849 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:51:51,849 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:51:52,230 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f6g5 c7h2 c7b8 f6f7 f6e7      E: -4.9 -4.9 -5.12 -5.17 -4.99             B: c7b8...
2025-08-27 18:51:54,826 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:51:55,119 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e5d3 b1d2 c2a4 f2f4 f1e2      E: -0.32 -0.08 0.12 -0.31 0.17             B: f1e2...
2025-08-27 18:51:57,187 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:51:57,483 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g2g3 h5g6 f3d4 f3g5 a1d1      E: 7.59 8.47 7.71 7.81 8.31                B: h5g6...
2025-08-27 18:51:59,568 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:51:59,891 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b1c1 c2a3 c2e1 b2c3 d2d1      E: -0.1 -0.13 -0.15 -0.13 -0.14            B: b1c1...
2025-08-27 18:52:02,144 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:52:02,450 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d4d3 a6c5 h8h3 b6a5 e7h4      E: -3.95 -3.93 -3.94 -3.96 -3.63           B: b6a5...
2025-08-27 18:52:04,609 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:52:04,896 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b5d4 b2b3 f1e1 d1c2 e5g7      E: -3.6 -3.89 -4.05 -3.79 -2.95            B: e5g7...
2025-08-27 18:52:06,901 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:52:07,262 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f6g6 f6e5                     E: -999.98 -0.28                           B: f6e5...
2025-08-27 18:52:09,832 | grpo_training | DEBUG | Processing sample 8/8: A task
2025-08-27 18:52:09,994 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 7r/ppkn1R1p/b1p3p1/8/8/B1P2N2/PP4PP/4R1K1 w - - 5 36+0.001+0+0...
2025-08-27 18:52:11,128 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:52:11,128 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 11) ===
2025-08-27 18:52:11,128 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:52:11,128 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.311, std=0.260
2025-08-27 18:52:11,128 | grpo_training | DEBUG | Reward distribution: {'0.2': 56, '1.0': 8}
2025-08-27 18:52:11,128 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:52:11,128 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:52:11,128 | grpo_training | DEBUG | Completion: M: f6g5 c7h2 c7b8 f6f7 f6e7      E: -4.9 -4.9 -5.12 -5.17 -4.99             B: c7b8...
2025-08-27 18:52:11,128 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:52:11,128 | grpo_training | DEBUG | Completion: M: c7b8 c7g3 f6g5 c7e5 c7h2      E: -4.44 -4.44 -4.36 -5.07 -4.48           B: c7b8...
2025-08-27 18:52:11,128 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:52:11,128 | grpo_training | DEBUG | Completion: M: c7h2 c7g3 c7d6 c7b8 f6e7      E: -7.37 -6.69 -5.63 -4.6 -7.12            B: c7h2...
2025-08-27 18:52:11,338 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001718

Step 11 | Time: 19.66s
2025-08-27 18:52:11,524 | grpo_training | INFO | 
Step 11 | Time: 19.66s
  Loss: -0.0618
2025-08-27 18:52:11,524 | grpo_training | INFO |   Loss: -0.0618
  PG Loss: -0.0000
2025-08-27 18:52:11,524 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.3938
2025-08-27 18:52:11,524 | grpo_training | INFO |   KL Divergence: -0.3938
2025-08-27 18:52:11,524 | grpo_training | DEBUG |   KL Forward: -0.3938
2025-08-27 18:52:11,524 | grpo_training | DEBUG |   KL Reverse: 0.7409
2025-08-27 18:52:11,524 | grpo_training | DEBUG |   KL Symmetric: 0.1736
2025-08-27 18:52:11,524 | grpo_training | DEBUG |   Ratio Outliers: 46.875%
2025-08-27 18:52:11,524 | grpo_training | DEBUG |   Gradient Norm: 0.2516
2025-08-27 18:52:11,626 | grpo_training | DEBUG | 
Step 12: Sampled 8 prompts
2025-08-27 18:52:11,626 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:52:11,626 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:52:11,942 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g7g6 h5h4 f8d6 f5e4 b7b6      E: 0.58 1.19 0.96 1.39 0.98                B: g7g6...
2025-08-27 18:52:14,122 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:52:14,398 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d4d5 c1d2 f1g3 c1e3 d1e2      E: 0.92 0.71 0.85 0.73 0.72                B: d4d5...
2025-08-27 18:52:16,348 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:52:16,668 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g7g5 g7g6 f8c8 e4f5 e4h4      E: 3.13 2.38 3.08 3.38 3.14                B: g7g6...
2025-08-27 18:52:18,926 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:52:19,308 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g2h1 g2f1 g2h2 h3g3           E: -7.86 -7.85 -7.84 -7.65                 B: h3g3...
2025-08-27 18:52:21,890 | grpo_training | DEBUG | Processing sample 5/8: A task
2025-08-27 18:52:22,046 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 5k2/8/2p5/2p4p/R1BP1BN1/5N2/1P3PPP/6K1 w - - 0 37+0.001+0+0...
2025-08-27 18:52:23,140 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:52:23,451 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d1f1 d1b1 d1d2 f3d2 d1e1      E: -7.62 -6.93 -1.18 -0.59 -7.74           B: f3d2...
2025-08-27 18:52:25,634 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:52:25,973 | grpo_training | DEBUG |   K=1, Reward=-0.300, Completion: +d3d7+d8e8 a1a8 e7e6 f3e5 f8e7 g5f6 h7h8 f6g5 e7d8 d3d7+R1bqkb1r/1p1Q2p1/4p3/1Np1NpB1/2B4p/8/1PP2PPP...
2025-08-27 18:52:28,007 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:52:28,309 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g1g2 d3b1 d3c2 f1e1 f1d1      E: -0.38 -0.28 -0.09 -0.22 -0.4            B: d3c2...
2025-08-27 18:52:30,408 | grpo_training | DEBUG | 
Advantages: mean=-0.000, std=1.000
2025-08-27 18:52:30,409 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 12) ===
2025-08-27 18:52:30,409 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:52:30,409 | grpo_training | DEBUG | Rewards: min=-0.300, max=1.000, mean=0.287, std=0.290
2025-08-27 18:52:30,409 | grpo_training | DEBUG | Reward distribution: {'0.2': 53, '1.0': 8, '-0.3': 3}
2025-08-27 18:52:30,409 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:52:30,409 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:52:30,409 | grpo_training | DEBUG | Completion: M: g7g6 h5h4 f8d6 f5e4 b7b6      E: 0.58 1.19 0.96 1.39 0.98                B: g7g6...
2025-08-27 18:52:30,409 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:52:30,409 | grpo_training | DEBUG | Completion: M: f5e4 f8e7 b7b6 f8d6 h5h4      E: 2.17 1.7 2.05 1.63 2.39                 B: f8d6...
2025-08-27 18:52:30,409 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:52:30,409 | grpo_training | DEBUG | Completion: M: h5h4 f8d6 f8e7 b7b6 f7f6      E: 1.68 1.29 1.4 1.48 1.28                 B: f7f6...
2025-08-27 18:52:30,613 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001374

Step 12 | Time: 19.16s
2025-08-27 18:52:30,796 | grpo_training | INFO | 
Step 12 | Time: 19.16s
  Loss: 0.1557
2025-08-27 18:52:30,796 | grpo_training | INFO |   Loss: 0.1557
  PG Loss: 0.2133
2025-08-27 18:52:30,796 | grpo_training | INFO |   PG Loss: 0.2133
  KL Divergence: -0.3681
2025-08-27 18:52:30,796 | grpo_training | INFO |   KL Divergence: -0.3681
2025-08-27 18:52:30,796 | grpo_training | DEBUG |   KL Forward: -0.3681
2025-08-27 18:52:30,796 | grpo_training | DEBUG |   KL Reverse: 0.4690
2025-08-27 18:52:30,796 | grpo_training | DEBUG |   KL Symmetric: 0.0504
2025-08-27 18:52:30,796 | grpo_training | DEBUG |   Ratio Outliers: 43.750%
2025-08-27 18:52:30,796 | grpo_training | DEBUG |   Gradient Norm: 121.4447
2025-08-27 18:52:30,894 | grpo_training | DEBUG | 
Step 13: Sampled 8 prompts
2025-08-27 18:52:30,894 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:52:30,894 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:52:31,216 | grpo_training | DEBUG |   K=1, Reward=-0.300, Completion: +c7c5+d5e5 h8e8 d1d4 d8d6 d4d6 b8d7 d6d7 e8d7 c4f1 c7c5+r1b5/3kpp1p/p5p1/1pp1Q3/8/8/PPP2PPP/RNB1KBNR...
2025-08-27 18:52:33,463 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:52:33,797 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e2d1 e2d2 g1g4 e2g4 e2d3      E: 3.9 4.02 4.87 4.0 5.0                   B: e2d1...
2025-08-27 18:52:36,059 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:52:36,408 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8f8 g8h8                     E: 0.72 0.57                               B: g8h8...
2025-08-27 18:52:38,853 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:52:39,137 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a2a4 f1e1 b2b4 b1d2 c4b3      E: 0.27 0.2 0.24 0.21 0.25                 B: a2a4...
2025-08-27 18:52:41,091 | grpo_training | DEBUG | Processing sample 5/8: A task
2025-08-27 18:52:41,276 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 6Q1/pr1N4/1p1p2pk/3P4/4P3/P1B2NPB/1P3P2/R3K2R w KQ - 7 33+0.001+0+0...
2025-08-27 18:52:42,566 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:52:42,834 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d3d8 d3d7 c4e6 b5c7 e5f7      E: 999.99 999.97 999.97 999.97 999.95      B: d3d8...
2025-08-27 18:52:44,725 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:52:45,011 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a7a6 e6g4 e6d7 b8a8 h7h6      E: -0.89 -1.02 -1.16 -0.95 -1.14           B: e6d7...
2025-08-27 18:52:47,044 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:52:47,355 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b8c8 f7f6 b8d8 a6b7 c6c4      E: -5.19 -4.76 -5.19 -4.97 -5.19           B: b8c8...
2025-08-27 18:52:49,535 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:52:49,535 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 13) ===
2025-08-27 18:52:49,535 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:52:49,536 | grpo_training | DEBUG | Rewards: min=-0.300, max=1.000, mean=0.247, std=0.330
2025-08-27 18:52:49,536 | grpo_training | DEBUG | Reward distribution: {'-0.3': 8, '0.2': 48, '1.0': 8}
2025-08-27 18:52:49,536 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:52:49,536 | grpo_training | DEBUG | 
[Sample 1] Reward: -0.300
2025-08-27 18:52:49,536 | grpo_training | DEBUG | Completion: +c7c5+d5e5 h8e8 d1d4 d8d6 d4d6 b8d7 d6d7 e8d7 c4f1 c7c5+r1b5/3kpp1p/p5p1/1pp1Q3/8/8/PPP2PPP/RNB1KBNR w KQ - 0 12+0.001+0+0...
2025-08-27 18:52:49,536 | grpo_training | DEBUG | 
[Sample 2] Reward: -0.300
2025-08-27 18:52:49,536 | grpo_training | DEBUG | Completion: +b5b4+d6c6 g7f6 c6b7 a8a7 b7b8 e8d7 b8f6 a7a8 f6e5 b5b4+r1b5/2pkpp1p/p5p1/4Q3/1p6/8/PPP2PPP/RNB1KBNR w KQ - 0 12+0.001+0+0...
2025-08-27 18:52:49,536 | grpo_training | DEBUG | 
[Sample 3] Reward: -0.300
2025-08-27 18:52:49,536 | grpo_training | DEBUG | Completion: +c7c6+d4g7 d8e8 g7h8q e8d8 h8f8 d8d7 f8g8 b8a8 g8g7 c7c6+r1b5/3kpp1p/p1p3p1/1p2Q3/8/8/PPP2PPP/RNB1KBNR w KQ - 0 12+0.001+0+0...
2025-08-27 18:52:49,742 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001100

Step 13 | Time: 19.02s
2025-08-27 18:52:49,923 | grpo_training | INFO | 
Step 13 | Time: 19.02s
  Loss: -0.0549
2025-08-27 18:52:49,924 | grpo_training | INFO |   Loss: -0.0549
  PG Loss: -0.0000
2025-08-27 18:52:49,924 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.4045
2025-08-27 18:52:49,924 | grpo_training | INFO |   KL Divergence: -0.4045
2025-08-27 18:52:49,924 | grpo_training | DEBUG |   KL Forward: -0.4045
2025-08-27 18:52:49,924 | grpo_training | DEBUG |   KL Reverse: 0.5438
2025-08-27 18:52:49,924 | grpo_training | DEBUG |   KL Symmetric: 0.0696
2025-08-27 18:52:49,924 | grpo_training | DEBUG |   Ratio Outliers: 28.125%
2025-08-27 18:52:49,924 | grpo_training | DEBUG |   Gradient Norm: 0.2369
2025-08-27 18:52:50,023 | grpo_training | DEBUG | 
Step 14: Sampled 8 prompts
2025-08-27 18:52:50,023 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:52:50,023 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:52:50,382 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8h8 g8f8                     E: 0.4 1.09                                B: g8h8...
2025-08-27 18:52:52,826 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:52:53,111 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e1d1 e1a1 d3b4 e1c1 f2e2      E: 0.39 0.37 1.03 0.4 0.34                 B: d3b4...
2025-08-27 18:52:55,117 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:52:55,404 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g1f3 c1e3 e4e5 d4d5 f1e2      E: 1.18 1.02 1.68 1.07 0.92                B: e4e5...
2025-08-27 18:52:57,407 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:52:57,689 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8e7 a5c4 d8f6 g8f6 b7e4      E: -9.57 -999.97 -8.32 -8.37 -9.9          B: a5c4...
2025-08-27 18:52:59,696 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:53:00,059 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d4d3 b6a5 b6c5 h6h5 f6g7      E: -1.97 5.5 6.05 -2.62 3.58               B: h6h5...
2025-08-27 18:53:02,590 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:53:02,905 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f3e5 c5c6 f3h2 f3e1 f3d2      E: 0.28 0.0 0.17 -0.05 0.34                B: f3d2...
2025-08-27 18:53:05,098 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:53:05,401 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d6e6 g8d8 g6h6 g8f8 e4e3      E: -0.81 -1.14 -1.17 -0.95 -0.61           B: g6h6...
2025-08-27 18:53:07,544 | grpo_training | DEBUG | Processing sample 8/8: A task
2025-08-27 18:53:07,740 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: rn2kb1r/1N1p1pp1/b1p1pn1p/7P/1p1PPNP1/3BBQ2/PPP2P2/2KR3R w kq - 0 16+0.001+0+0...
2025-08-27 18:53:09,121 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:53:09,121 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 14) ===
2025-08-27 18:53:09,121 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:53:09,122 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.311, std=0.260
2025-08-27 18:53:09,122 | grpo_training | DEBUG | Reward distribution: {'0.2': 56, '1.0': 8}
2025-08-27 18:53:09,122 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:53:09,122 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:53:09,122 | grpo_training | DEBUG | Completion: M: g8h8 g8f8                     E: 0.4 1.09                                B: g8h8...
2025-08-27 18:53:09,122 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:53:09,122 | grpo_training | DEBUG | Completion: M: g8h8 g8f8                     E: 0.55 0.54                               B: g8f8...
2025-08-27 18:53:09,122 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:53:09,122 | grpo_training | DEBUG | Completion: M: g8f8 g8h8                     E: 0.69 0.42                               B: g8h8...
2025-08-27 18:53:09,333 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 14 | Time: 19.49s
2025-08-27 18:53:09,520 | grpo_training | INFO | 
Step 14 | Time: 19.49s
  Loss: -0.0574
2025-08-27 18:53:09,520 | grpo_training | INFO |   Loss: -0.0574
  PG Loss: -0.0000
2025-08-27 18:53:09,520 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.3971
2025-08-27 18:53:09,520 | grpo_training | INFO |   KL Divergence: -0.3971
2025-08-27 18:53:09,520 | grpo_training | DEBUG |   KL Forward: -0.3971
2025-08-27 18:53:09,520 | grpo_training | DEBUG |   KL Reverse: 0.5050
2025-08-27 18:53:09,520 | grpo_training | DEBUG |   KL Symmetric: 0.0539
2025-08-27 18:53:09,520 | grpo_training | DEBUG |   Ratio Outliers: 32.812%
2025-08-27 18:53:09,520 | grpo_training | DEBUG |   Gradient Norm: 0.1840
2025-08-27 18:53:09,629 | grpo_training | DEBUG | 
Step 15: Sampled 8 prompts
2025-08-27 18:53:09,630 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:53:09,630 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:53:09,922 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d4c5 a2a4 f1d1 b1d2 f1c1      E: 0.04 0.01 0.07 -0.02 0.08               B: f1d1...
2025-08-27 18:53:11,942 | grpo_training | DEBUG | Processing sample 2/8: A task
2025-08-27 18:53:12,137 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 1r4q1/6p1/1p1p1nNp/p1p5/Q1b1PR1k/2N4P/PP3PP1/3R2K1 b - - 14 32+0.001+0+0...
2025-08-27 18:53:13,502 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:53:13,786 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f2d2 d3c5 f2f3 d3b4 f2e2      E: 0.34 0.57 0.34 0.37 0.27                B: d3c5...
2025-08-27 18:53:15,790 | grpo_training | DEBUG | Processing sample 4/8: A task
2025-08-27 18:53:15,944 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 8/2p5/p1P3P1/1p6/5Q2/7p/PPP1RPkP/2K5 b - - 10 42+0.001+0+0...
2025-08-27 18:53:17,018 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:53:17,307 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f5c2 b6d8 b6c7 b6b4 b6b3      E: -7.7 -7.11 -7.19 -7.44 -7.85            B: b6b3...
2025-08-27 18:53:19,322 | grpo_training | DEBUG | Processing sample 6/8: A task
2025-08-27 18:53:19,507 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 6Q1/pr1N4/1p1p2pk/3P4/4P3/P1B2NPB/1P3P2/R3K2R w KQ - 7 33+0.001+0+0...
2025-08-27 18:53:20,800 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:53:21,123 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g1g2 g1g4 e2g4 h7h6 e2g2      E: 3.64 4.84 4.19 2.95 4.14                B: h7h6...
2025-08-27 18:53:23,395 | grpo_training | DEBUG | Processing sample 8/8: A task
2025-08-27 18:53:23,557 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
2025-08-27 18:53:24,696 | grpo_training | DEBUG | 
Advantages: mean=-0.000, std=1.000
2025-08-27 18:53:24,697 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 15) ===
2025-08-27 18:53:24,697 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:53:24,697 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.602, std=0.391
2025-08-27 18:53:24,697 | grpo_training | DEBUG | Reward distribution: {'0.2': 32, '1.0': 31, '0.8': 1}
2025-08-27 18:53:24,697 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:53:24,697 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:53:24,697 | grpo_training | DEBUG | Completion: M: d4c5 a2a4 f1d1 b1d2 f1c1      E: 0.04 0.01 0.07 -0.02 0.08               B: f1d1...
2025-08-27 18:53:24,697 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:53:24,697 | grpo_training | DEBUG | Completion: M: f1c1 b1d2 f3e5 f1d1 d4c5      E: 0.06 0.12 0.02 0.03 0.06                B: b1d2...
2025-08-27 18:53:24,697 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:53:24,697 | grpo_training | DEBUG | Completion: M: a2a4 b1d2 f1d1 f3e5 f1c1      E: 0.24 0.28 0.1 0.09 0.2                  B: b1d2...
2025-08-27 18:53:24,906 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 15 | Time: 15.46s
2025-08-27 18:53:25,097 | grpo_training | INFO | 
Step 15 | Time: 15.46s
  Loss: 0.0412
2025-08-27 18:53:25,097 | grpo_training | INFO |   Loss: 0.0412
  PG Loss: 0.0796
2025-08-27 18:53:25,097 | grpo_training | INFO |   PG Loss: 0.0796
  KL Divergence: -0.4856
2025-08-27 18:53:25,097 | grpo_training | INFO |   KL Divergence: -0.4856
2025-08-27 18:53:25,097 | grpo_training | DEBUG |   KL Forward: -0.4856
2025-08-27 18:53:25,097 | grpo_training | DEBUG |   KL Reverse: 0.6706
2025-08-27 18:53:25,097 | grpo_training | DEBUG |   KL Symmetric: 0.0925
2025-08-27 18:53:25,097 | grpo_training | DEBUG |   Ratio Outliers: 39.062%
2025-08-27 18:53:25,097 | grpo_training | DEBUG |   Gradient Norm: 1.4989
2025-08-27 18:53:25,205 | grpo_training | DEBUG | 
Step 16: Sampled 8 prompts
2025-08-27 18:53:25,205 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:53:25,205 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:53:25,484 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d1e2 c2c3 d3a6 d2b3 f3e5      E: 0.14 0.22 0.24 0.09 0.21                B: c2c3...
2025-08-27 18:53:27,406 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:53:27,763 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c7d8 c7b7 c7d7 c7b8 c7c8      E: -999.93 -999.91 -999.89 -999.93 -999.93 B: c7d7...
2025-08-27 18:53:30,332 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:53:30,606 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b8d7 a7a5 c6c5 d5e4 d6e7      E: -0.12 -0.16 -0.18 -0.21 -0.17           B: d5e4...
2025-08-27 18:53:32,522 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:53:32,811 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c6a5 d8e7 b4e7 b4d6 a8c8      E: 0.34 0.27 0.33 0.33 0.35                B: d8e7...
2025-08-27 18:53:34,839 | grpo_training | DEBUG | Processing sample 5/8: A task
2025-08-27 18:53:35,037 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 1rbqkb1r/2p1pp2/p1Pp1P2/6Bp/2B3p1/2N5/PP2QPPP/R3K1NR b KQk - 1 11+0.001+0+0...
2025-08-27 18:53:36,423 | grpo_training | DEBUG | Processing sample 6/8: A task
2025-08-27 18:53:36,625 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r1bqk2r/2Bpbppp/1pp1p3/7P/N1PPP1n1/5N2/PPQ2PP1/R3KB1R b KQkq - 0 11+0.001+0+0...
2025-08-27 18:53:38,030 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:53:38,411 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d5c4 d5d4 d5d6 d5c6           E: -6.89 -6.88 -5.56 -7.42                 B: d5d6...
2025-08-27 18:53:41,076 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:53:41,372 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c1e3 b1c3 b2b4 d1a4 f1d3      E: 2.96 3.15 3.13 3.15 3.09                B: d1a4...
2025-08-27 18:53:43,458 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:53:43,459 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 16) ===
2025-08-27 18:53:43,459 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:53:43,459 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.409, std=0.341
2025-08-27 18:53:43,459 | grpo_training | DEBUG | Reward distribution: {'0.2': 48, '1.0': 16}
2025-08-27 18:53:43,459 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:53:43,459 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:53:43,459 | grpo_training | DEBUG | Completion: M: d1e2 c2c3 d3a6 d2b3 f3e5      E: 0.14 0.22 0.24 0.09 0.21                B: c2c3...
2025-08-27 18:53:43,459 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:53:43,459 | grpo_training | DEBUG | Completion: M: f3e5 f3g5 h3h4 d1e2 d3a6      E: 0.24 0.15 0.11 0.2 0.23                 B: f3e5...
2025-08-27 18:53:43,459 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:53:43,459 | grpo_training | DEBUG | Completion: M: c2c3 d3a6 d3e2 h3h4 f3e5      E: 0.17 0.26 0.17 0.16 0.21                B: d3a6...
2025-08-27 18:53:43,672 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 16 | Time: 18.65s
2025-08-27 18:53:43,863 | grpo_training | INFO | 
Step 16 | Time: 18.65s
  Loss: -0.0456
2025-08-27 18:53:43,863 | grpo_training | INFO |   Loss: -0.0456
  PG Loss: -0.0000
2025-08-27 18:53:43,863 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.4034
2025-08-27 18:53:43,863 | grpo_training | INFO |   KL Divergence: -0.4034
2025-08-27 18:53:43,863 | grpo_training | DEBUG |   KL Forward: -0.4034
2025-08-27 18:53:43,863 | grpo_training | DEBUG |   KL Reverse: 0.6904
2025-08-27 18:53:43,863 | grpo_training | DEBUG |   KL Symmetric: 0.1435
2025-08-27 18:53:43,863 | grpo_training | DEBUG |   Ratio Outliers: 23.438%
2025-08-27 18:53:43,863 | grpo_training | DEBUG |   Gradient Norm: 0.1868
2025-08-27 18:53:43,972 | grpo_training | DEBUG | 
Step 17: Sampled 8 prompts
2025-08-27 18:53:43,972 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:53:43,972 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:53:44,261 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8g7 e5e4 c8h3 c6d5 c8d7      E: -1.09 1.34 -0.66 1.2 -1.1               B: c8d7...
2025-08-27 18:53:46,198 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:53:46,532 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h8h7 g5g4 h5h4 f8f7 h8g7      E: -3.36 -2.92 -2.63 -3.01 -2.58           B: h8h7...
2025-08-27 18:53:48,896 | grpo_training | DEBUG | Processing sample 3/8: A task
2025-08-27 18:53:49,095 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 1rbqkb1r/2p1pp2/p1Pp1P2/6Bp/2B3p1/2N5/PP2QPPP/R3K1NR b KQk - 1 11+0.001+0+0...
2025-08-27 18:53:50,482 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:53:50,873 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f7a7 f7f6 h3h2 f7c7 f7g7      E: 0.0 0.0 0.0 0.0 0.0                     B: f7g7...
2025-08-27 18:53:53,565 | grpo_training | DEBUG | Processing sample 5/8: A task
2025-08-27 18:53:53,731 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: Qnbqk2r/5ppp/3bpn2/p7/2PP4/8/PP3PPP/RNBQKBNR w KQk - 1 8+0.001+0+0...
2025-08-27 18:53:54,894 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:53:55,190 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c2b2 c3c4 h2g1 d1c1 d1f1      E: -0.94 -1.05 -1.01 -1.03 -0.95           B: c2b2...
2025-08-27 18:53:57,272 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:53:57,568 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b1d2 e5d3 c1e3 f1d3 f2f4      E: -0.36 -0.28 -0.17 -0.3 -0.36            B: c1e3...
2025-08-27 18:53:59,633 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:53:59,961 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f6g6 g4f2 f6f3 h6h5 f6b2      E: -2.41 -4.17 -2.3 -2.86 -3.68            B: g4f2...
2025-08-27 18:54:02,234 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:54:02,234 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 17) ===
2025-08-27 18:54:02,234 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:54:02,234 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.409, std=0.341
2025-08-27 18:54:02,234 | grpo_training | DEBUG | Reward distribution: {'0.2': 48, '1.0': 16}
2025-08-27 18:54:02,234 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:54:02,234 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:54:02,234 | grpo_training | DEBUG | Completion: M: g8g7 e5e4 c8h3 c6d5 c8d7      E: -1.09 1.34 -0.66 1.2 -1.1               B: c8d7...
2025-08-27 18:54:02,234 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:54:02,234 | grpo_training | DEBUG | Completion: M: c6d5 f8b4 g8g7 f8e7 f6h7      E: -3.35 -3.4 -2.55 -1.72 -2.2             B: f8b4...
2025-08-27 18:54:02,234 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:54:02,234 | grpo_training | DEBUG | Completion: M: e5d4 c8d7 e5e4 f8e7 f8b4      E: 0.43 -1.16 0.13 -1.06 -1.26             B: f8b4...
2025-08-27 18:54:02,449 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 17 | Time: 18.66s
2025-08-27 18:54:02,637 | grpo_training | INFO | 
Step 17 | Time: 18.66s
  Loss: -0.0602
2025-08-27 18:54:02,638 | grpo_training | INFO |   Loss: -0.0602
  PG Loss: -0.0000
2025-08-27 18:54:02,638 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.5438
2025-08-27 18:54:02,638 | grpo_training | INFO |   KL Divergence: -0.5438
2025-08-27 18:54:02,638 | grpo_training | DEBUG |   KL Forward: -0.5438
2025-08-27 18:54:02,638 | grpo_training | DEBUG |   KL Reverse: 0.7907
2025-08-27 18:54:02,638 | grpo_training | DEBUG |   KL Symmetric: 0.1234
2025-08-27 18:54:02,638 | grpo_training | DEBUG |   Ratio Outliers: 37.500%
2025-08-27 18:54:02,638 | grpo_training | DEBUG |   Gradient Norm: 0.2389
2025-08-27 18:54:02,747 | grpo_training | DEBUG | 
Step 18: Sampled 8 prompts
2025-08-27 18:54:02,748 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:54:02,748 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:54:03,058 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a7a5 h6h5 a8b8 g5g4 b7c6      E: 2.76 3.08 3.09 3.11 3.06                B: a7a5...
2025-08-27 18:54:05,182 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:54:05,542 | grpo_training | DEBUG |   K=1, Reward=-0.300, Completion: +b8d8+d3c2 g8f6 e4e5 f6h5 c2h7 c8b7 h7d3 h5f4 d3f3 b8d8+3rkb1r/1bqn1ppp/p1p1p3/2PpP3/Np1P1N2/4BB2/PP...
2025-08-27 18:54:07,647 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:54:07,937 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a1c1 d3f4 a2a4 f1e1 d1d2      E: 0.4 0.37 0.43 0.4 0.39                  B: a2a4...
2025-08-27 18:54:09,946 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:54:10,324 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c4c3 a8d5 a8h1 a8f3 a8e4      E: 0.0 0.0 0.0 0.0 0.0                     B: c4c3...
2025-08-27 18:54:12,961 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:54:13,282 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c2c4 e2e4 b1c3 d2d4 g1f3      E: 0.22 0.48 0.31 0.3 0.17                 B: e2e4...
2025-08-27 18:54:15,528 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:54:15,818 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a4a5 a4a2 a4e8 f7f5 a4c6      E: 1.89 -4.12 1.34 -2.81 0.83              B: a4a2...
2025-08-27 18:54:17,854 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:54:18,214 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g3h3 g7h6 h7h5 a7a5 g7g6      E: -1.95 -2.67 -2.67 -2.73 -2.44           B: a7a5...
2025-08-27 18:54:20,745 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:54:21,124 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h3g3 g2h2 g2f1 g2h1           E: -8.04 -5.84 -8.5 -7.98                  B: g2h2...
2025-08-27 18:54:23,739 | grpo_training | DEBUG | 
Advantages: mean=-0.000, std=1.000
2025-08-27 18:54:23,739 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 18) ===
2025-08-27 18:54:23,739 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:54:23,739 | grpo_training | DEBUG | Rewards: min=-0.300, max=0.212, mean=0.188, std=0.108
2025-08-27 18:54:23,739 | grpo_training | DEBUG | Reward distribution: {'0.2': 61, '-0.3': 3}
2025-08-27 18:54:23,739 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:54:23,739 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:54:23,739 | grpo_training | DEBUG | Completion: M: a7a5 h6h5 a8b8 g5g4 b7c6      E: 2.76 3.08 3.09 3.11 3.06                B: a7a5...
2025-08-27 18:54:23,739 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:54:23,739 | grpo_training | DEBUG | Completion: M: a7a5 h8d8 h8b8 h6h5 e5f6      E: 2.75 2.76 2.56 2.83 2.72                B: h8b8...
2025-08-27 18:54:23,739 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:54:23,739 | grpo_training | DEBUG | Completion: M: e5f6 e5d6 a7a6 g5g4 a7a5      E: 2.63 2.66 2.81 2.65 2.62                B: a7a5...
2025-08-27 18:54:23,963 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 18 | Time: 21.40s
2025-08-27 18:54:24,161 | grpo_training | INFO | 
Step 18 | Time: 21.40s
  Loss: 6.7953
2025-08-27 18:54:24,162 | grpo_training | INFO |   Loss: 6.7953
  PG Loss: 6.8673
2025-08-27 18:54:24,162 | grpo_training | INFO |   PG Loss: 6.8673
  KL Divergence: -0.6215
2025-08-27 18:54:24,162 | grpo_training | INFO |   KL Divergence: -0.6215
2025-08-27 18:54:24,162 | grpo_training | DEBUG |   KL Forward: -0.6215
2025-08-27 18:54:24,162 | grpo_training | DEBUG |   KL Reverse: 1.4785
2025-08-27 18:54:24,162 | grpo_training | DEBUG |   KL Symmetric: 0.4285
2025-08-27 18:54:24,162 | grpo_training | DEBUG |   Ratio Outliers: 50.000%
2025-08-27 18:54:24,162 | grpo_training | DEBUG |   Gradient Norm: 3083.8281
2025-08-27 18:54:24,274 | grpo_training | DEBUG | 
Step 19: Sampled 8 prompts
2025-08-27 18:54:24,274 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:54:24,274 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:54:24,588 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a6b8 a6c5 g7g6 d4d3 e7e8      E: -4.06 -3.83 -4.18 -3.82 -3.89           B: g7g6...
2025-08-27 18:54:26,746 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:54:27,079 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g4e5 h6h5 f6b2 f6e7 f6g6      E: -2.28 -2.49 -2.3 -2.39 -1.7             B: h6h5...
2025-08-27 18:54:29,384 | grpo_training | DEBUG | Processing sample 3/8: A task
2025-08-27 18:54:29,557 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: Qnb1k1nr/p4ppp/2qbp3/8/2PP4/5N2/PP3PPP/RNBQKB1R b KQk - 2 8+0.001+0+0...
2025-08-27 18:54:30,771 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:54:31,059 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h1g1 f3e5 h5h6 f3g5 d2c1      E: 13.15 12.98 12.88 999.97 13.05          B: f3g5...
2025-08-27 18:54:33,066 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:54:33,344 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b4a5 f3e5 c2c4 b1c3 e1g1      E: 4.73 3.09 3.68 3.13 3.29                B: b4a5...
2025-08-27 18:54:35,301 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:54:35,633 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8f7 e6f7 e6f5 e6d5 b8e8      E: 2.21 1.99 1.99 2.04 2.13                B: e6f7...
2025-08-27 18:54:37,973 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:54:38,244 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h7h6 a8c8 h5g6 f8e8 e7d6      E: -0.18 0.16 0.24 -0.11 0.24              B: h7h6...
2025-08-27 18:54:40,158 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:54:40,476 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e2d1 e2d3 e2f1 e2e1           E: -999.99 -0.13 -999.99 -999.98           B: e2d3...
2025-08-27 18:54:42,751 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:54:42,752 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 19) ===
2025-08-27 18:54:42,752 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:54:42,752 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.311, std=0.260
2025-08-27 18:54:42,752 | grpo_training | DEBUG | Reward distribution: {'0.2': 56, '1.0': 8}
2025-08-27 18:54:42,752 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:54:42,752 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:54:42,752 | grpo_training | DEBUG | Completion: M: a6b8 a6c5 g7g6 d4d3 e7e8      E: -4.06 -3.83 -4.18 -3.82 -3.89           B: g7g6...
2025-08-27 18:54:42,752 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:54:42,752 | grpo_training | DEBUG | Completion: M: g7g6 a6c5 h8h3 c6d7 b6a5      E: -3.32 -3.38 -3.57 -4.08 -3.72           B: b6a5...
2025-08-27 18:54:42,752 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:54:42,752 | grpo_training | DEBUG | Completion: M: h8h4 h8e8 c8b8 c6f3 d4d3      E: -4.07 -4.04 -3.73 -3.83 -4.28           B: d4d3...
2025-08-27 18:54:42,947 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 19 | Time: 18.84s
2025-08-27 18:54:43,126 | grpo_training | INFO | 
Step 19 | Time: 18.84s
  Loss: -0.0550
2025-08-27 18:54:43,126 | grpo_training | INFO |   Loss: -0.0550
  PG Loss: -0.0000
2025-08-27 18:54:43,126 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.5203
2025-08-27 18:54:43,127 | grpo_training | INFO |   KL Divergence: -0.5203
2025-08-27 18:54:43,127 | grpo_training | DEBUG |   KL Forward: -0.5203
2025-08-27 18:54:43,127 | grpo_training | DEBUG |   KL Reverse: 1.1136
2025-08-27 18:54:43,127 | grpo_training | DEBUG |   KL Symmetric: 0.2966
2025-08-27 18:54:43,127 | grpo_training | DEBUG |   Ratio Outliers: 37.500%
2025-08-27 18:54:43,127 | grpo_training | DEBUG |   Gradient Norm: 0.2081
2025-08-27 18:54:43,226 | grpo_training | DEBUG | 
Step 20: Sampled 8 prompts
2025-08-27 18:54:43,226 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:54:43,226 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:54:43,623 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c7b8 c7c8 c7b6                E: -999.96 -999.97 -999.96                 B: c7c8...
2025-08-27 18:54:46,357 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:54:46,729 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b3b2 d5d4 h4h3 b3b4 g7h6      E: 0.0 -0.01 0.0 0.0 -0.02                 B: g7h6...
2025-08-27 18:54:49,319 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:54:49,659 | grpo_training | DEBUG |   K=1, Reward=-0.300, Completion: +h7g7+e7d8 f5h3 d8e8 d2g5 e8f8 g5g6 a7a5 g6h7 d5e5 h7g7+5kN1/5pQ1/1p2p3/p1p1r3/3P4/5N1B/PPP2PPP/R1B1...
2025-08-27 18:54:51,853 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:54:52,106 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f3e5 d1b1 a1c1 d1c2 d4e5      E: 10.66 10.37 10.38 10.42 11.17           B: d4e5...
2025-08-27 18:54:53,886 | grpo_training | DEBUG | Processing sample 5/8: A task
2025-08-27 18:54:54,090 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r2Q2nr/1p1kp2p/2p2Ppb/p5B1/3P2b1/2N2N2/PP2BPPP/2RQ1RK1 b - - 2 17+0.001+0+0...
2025-08-27 18:54:55,523 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:54:55,800 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f3e5 e1g1 d3a6 c2c4 d1e2      E: 0.13 0.13 0.19 0.06 0.15                B: d3a6...
2025-08-27 18:54:57,757 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:54:58,035 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b4a5 b1d2 e1g1 f3e5 c2c4      E: 4.95 3.21 4.27 4.14 3.97                B: b4a5...
2025-08-27 18:54:59,982 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:55:00,267 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h2h3 d2d3 f1e1 c3d5 c3a2      E: -0.27 -0.02 -0.02 -0.24 -0.12           B: c3a2...
2025-08-27 18:55:02,267 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=1.000
2025-08-27 18:55:02,267 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 20) ===
2025-08-27 18:55:02,267 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:55:02,267 | grpo_training | DEBUG | Rewards: min=-0.300, max=1.000, mean=0.271, std=0.308
2025-08-27 18:55:02,267 | grpo_training | DEBUG | Reward distribution: {'0.2': 51, '-0.3': 5, '1.0': 8}
2025-08-27 18:55:02,267 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:55:02,267 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:55:02,267 | grpo_training | DEBUG | Completion: M: c7b8 c7c8 c7b6                E: -999.96 -999.97 -999.96                 B: c7c8...
2025-08-27 18:55:02,267 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:55:02,267 | grpo_training | DEBUG | Completion: M: c7c8 c7b8 c7b6                E: -999.97 -999.96 -999.96                 B: c7c8...
2025-08-27 18:55:02,267 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:55:02,267 | grpo_training | DEBUG | Completion: M: c7c8 c7b8 c7b6                E: -999.97 -999.96 -999.96                 B: c7c8...
2025-08-27 18:55:02,483 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 20 | Time: 19.44s
2025-08-27 18:55:02,674 | grpo_training | INFO | 
Step 20 | Time: 19.44s
  Loss: -0.0382
2025-08-27 18:55:02,674 | grpo_training | INFO |   Loss: -0.0382
  PG Loss: 0.0233
2025-08-27 18:55:02,674 | grpo_training | INFO |   PG Loss: 0.0233
  KL Divergence: -0.5107
2025-08-27 18:55:02,674 | grpo_training | INFO |   KL Divergence: -0.5107
2025-08-27 18:55:02,674 | grpo_training | DEBUG |   KL Forward: -0.5107
2025-08-27 18:55:02,674 | grpo_training | DEBUG |   KL Reverse: 0.8540
2025-08-27 18:55:02,674 | grpo_training | DEBUG |   KL Symmetric: 0.1716
2025-08-27 18:55:02,674 | grpo_training | DEBUG |   Ratio Outliers: 43.750%
2025-08-27 18:55:02,674 | grpo_training | DEBUG |   Gradient Norm: 58.4320

=== EVALUATION (Step 20) ===
2025-08-27 18:55:02,674 | grpo_training | INFO | 
=== EVALUATION (Step 20) ===
Mean reward (last batch): 0.271
2025-08-27 18:55:02,674 | grpo_training | INFO | Mean reward (last batch): 0.271
Reward distribution: ['0.2', '0.2', '0.2', '0.2', '0.2', '0.2', '0.2', '0.2', '0.2', '0.2']
2025-08-27 18:55:02,674 | grpo_training | INFO | Reward distribution: ['0.2', '0.2', '0.2', '0.2', '0.2', '0.2', '0.2', '0.2', '0.2', '0.2']
2025-08-27 18:55:02,786 | grpo_training | DEBUG | 
Step 21: Sampled 8 prompts
2025-08-27 18:55:02,786 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:55:02,787 | grpo_training | DEBUG | Processing sample 1/8: A task
2025-08-27 18:55:02,993 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: rn2kb1r/1N1p1pp1/b1p1pn1p/7P/1p1PPNP1/3BBQ2/PPP2P2/2KR3R w kq - 0 16+0.001+0+0...
2025-08-27 18:55:04,375 | grpo_training | DEBUG | Processing sample 2/8: A task
2025-08-27 18:55:04,569 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 5r1r/1b1p1pp1/n3pn2/BN2N3/p1p1P3/P6P/1PP1BPP1/R2R2K1 b - - 0 23+0.001+0+0...
2025-08-27 18:55:05,931 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:55:06,247 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f1f2 f1g1 f1e1                E: -5.01 -6.02 -4.77                       B: f1e1...
2025-08-27 18:55:08,452 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:55:08,749 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h2g1 d2d3 g2h1 h2h1 c3c4      E: -0.77 -0.87 -0.85 -0.98 -0.67           B: c3c4...
2025-08-27 18:55:10,829 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:55:11,126 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d7d5 d7d6 g8f6 e7e6 c6c5      E: -0.56 -0.72 -0.65 -0.94 -0.88           B: e7e6...
2025-08-27 18:55:13,200 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:55:13,531 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8h7 g6e7 g4f3 g4h5 h6h5      E: 0.95 0.84 0.94 0.78 0.69                B: h6h5...
2025-08-27 18:55:15,856 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:55:16,152 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d2c3 d2e1 d2a5 c8c2 f7f5      E: -4.29 -1.36 1.02 0.0 -0.01              B: d2c3...
2025-08-27 18:55:18,217 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:55:18,516 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c7c5 f7f6 c8b7 a8b8 b5b4      E: -11.11 -14.14 -12.35 -14.16 -13.41      B: a8b8...
2025-08-27 18:55:20,621 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:55:20,621 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 21) ===
2025-08-27 18:55:20,621 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:55:20,621 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.409, std=0.341
2025-08-27 18:55:20,621 | grpo_training | DEBUG | Reward distribution: {'1.0': 16, '0.2': 48}
2025-08-27 18:55:20,621 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:55:20,621 | grpo_training | DEBUG | 
[Sample 1] Reward: 1.000
2025-08-27 18:55:20,621 | grpo_training | DEBUG | Completion: rn2kb1r/1N1p1pp1/b1p1pn1p/7P/1p1PPNP1/3BBQ2/PPP2P2/2KR3R w kq - 0 16+0.001+0+0...
2025-08-27 18:55:20,621 | grpo_training | DEBUG | 
[Sample 2] Reward: 1.000
2025-08-27 18:55:20,621 | grpo_training | DEBUG | Completion: rn2kb1r/1N1p1pp1/b1p1pn1p/7P/1p1PPNP1/3BBQ2/PPP2P2/2KR3R w kq - 0 16+0.001+0+0...
2025-08-27 18:55:20,621 | grpo_training | DEBUG | 
[Sample 3] Reward: 1.000
2025-08-27 18:55:20,621 | grpo_training | DEBUG | Completion: rn2kb1r/1N1p1pp1/b1p1pn1p/7P/1p1PPNP1/3BBQ2/PPP2P2/2KR3R w kq - 0 16+0.001+0+0...
2025-08-27 18:55:20,831 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 21 | Time: 18.22s
2025-08-27 18:55:21,019 | grpo_training | INFO | 
Step 21 | Time: 18.22s
  Loss: -0.0530
2025-08-27 18:55:21,019 | grpo_training | INFO |   Loss: -0.0530
  PG Loss: -0.0000
2025-08-27 18:55:21,019 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.4662
2025-08-27 18:55:21,019 | grpo_training | INFO |   KL Divergence: -0.4662
2025-08-27 18:55:21,019 | grpo_training | DEBUG |   KL Forward: -0.4662
2025-08-27 18:55:21,019 | grpo_training | DEBUG |   KL Reverse: 0.9586
2025-08-27 18:55:21,019 | grpo_training | DEBUG |   KL Symmetric: 0.2462
2025-08-27 18:55:21,019 | grpo_training | DEBUG |   Ratio Outliers: 51.562%
2025-08-27 18:55:21,019 | grpo_training | DEBUG |   Gradient Norm: 0.2834
2025-08-27 18:55:21,131 | grpo_training | DEBUG | 
Step 22: Sampled 8 prompts
2025-08-27 18:55:21,132 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:55:21,132 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:55:21,413 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d1e2 a2a3 c4d3 a2a4 a1c1      E: 0.38 0.38 0.35 0.29 0.34                B: a2a3...
2025-08-27 18:55:23,342 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:55:23,626 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d3c1 d3b4 e1d1 f2e2 f2d2      E: 0.39 -0.04 0.38 -0.07 -0.05             B: d3c1...
2025-08-27 18:55:25,621 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:55:25,968 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h5h6 e2f3 c8e7 e4e5 c8a7      E: 0.34 0.41 1.34 0.34 1.21                B: c8e7...
2025-08-27 18:55:28,445 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:55:28,821 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b7b6 b7h7 g4h5 h2h4 b7d7      E: 0.0 -0.1 -0.05 -0.04 -0.01              B: b7b6...
2025-08-27 18:55:31,425 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:55:31,749 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b1c1 b2c3 a4a5 c2a1 b2c1      E: 0.0 -0.01 -0.27 -0.24 0.0               B: b2c1...
2025-08-27 18:55:33,995 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:55:34,326 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e8f7                          E: -999.97                                 B: e8f7...
2025-08-27 18:55:36,652 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:55:37,028 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h3g3 g2f1 g2h2 g2h1           E: -8.35 -9.06 -6.44 -8.35                 B: g2h2...
2025-08-27 18:55:39,627 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:55:39,917 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g8h8 c8g4 e7g6 e5f6 e7d5      E: -3.37 -2.82 -3.16 -3.02 -2.91           B: e7g6...
2025-08-27 18:55:41,961 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=1.000
2025-08-27 18:55:41,961 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 22) ===
2025-08-27 18:55:41,961 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:55:41,962 | grpo_training | DEBUG | Rewards: min=0.199, max=0.212, mean=0.212, std=0.002
2025-08-27 18:55:41,969 | grpo_training | DEBUG | Reward distribution: {'0.2': 64}
2025-08-27 18:55:41,969 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:55:41,969 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:55:41,969 | grpo_training | DEBUG | Completion: M: d1e2 a2a3 c4d3 a2a4 a1c1      E: 0.38 0.38 0.35 0.29 0.34                B: a2a3...
2025-08-27 18:55:41,969 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:55:41,969 | grpo_training | DEBUG | Completion: M: f1e1 d1e2 c4e2 e3e4 a2a4      E: 0.2 0.16 0.18 0.16 0.09                 B: f1e1...
2025-08-27 18:55:41,969 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:55:41,969 | grpo_training | DEBUG | Completion: M: d1e2 a1c1 c4d3 a2a4 f3e5      E: 0.34 0.28 0.28 0.2 0.26                 B: d1e2...
2025-08-27 18:55:42,167 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 22 | Time: 21.21s
2025-08-27 18:55:42,348 | grpo_training | INFO | 
Step 22 | Time: 21.21s
  Loss: 0.0830
2025-08-27 18:55:42,348 | grpo_training | INFO |   Loss: 0.0830
  PG Loss: 0.1384
2025-08-27 18:55:42,348 | grpo_training | INFO |   PG Loss: 0.1384
  KL Divergence: -0.4396
2025-08-27 18:55:42,348 | grpo_training | INFO |   KL Divergence: -0.4396
2025-08-27 18:55:42,348 | grpo_training | DEBUG |   KL Forward: -0.4396
2025-08-27 18:55:42,348 | grpo_training | DEBUG |   KL Reverse: 0.6312
2025-08-27 18:55:42,348 | grpo_training | DEBUG |   KL Symmetric: 0.0958
2025-08-27 18:55:42,348 | grpo_training | DEBUG |   Ratio Outliers: 45.312%
2025-08-27 18:55:42,348 | grpo_training | DEBUG |   Gradient Norm: 153.3507
2025-08-27 18:55:42,449 | grpo_training | DEBUG | 
Step 23: Sampled 8 prompts
2025-08-27 18:55:42,449 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:55:42,449 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:55:42,729 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h8h7 f1e1 f3g5 h1g1 f3e5      E: 999.97 999.97 999.96 999.98 999.97      B: h1g1...
2025-08-27 18:55:44,738 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:55:45,077 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b5d4 e6d7 d5e4 e6e7 e6f6      E: 0.16 -0.31 -0.04 -0.28 -0.27            B: e6d7...
2025-08-27 18:55:47,484 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:55:47,760 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f1e3 f1g3 d4c5 a2a4 d4d5      E: 0.52 0.48 0.38 0.56 0.49                B: d4d5...
2025-08-27 18:55:49,711 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:55:50,056 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e4d4 e4e2 d8d7 e4e3 e4e7      E: -2.17 -1.63 -2.27 -1.43 -1.9            B: d8d7...
2025-08-27 18:55:52,459 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:55:52,852 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h5e5 f4g3 h5h8 f4e4 h5h7      E: 0.09 0.03 0.08 0.08 0.08                B: h5e5...
2025-08-27 18:55:55,669 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:55:56,046 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h1g1                          E: 0.0                                     B: h1g1...
2025-08-27 18:55:58,681 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:55:59,015 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g6e7 g6h8 g8h7 h6h5 g4c8      E: 0.56 0.35 0.4 0.64 0.32                 B: g4c8...
2025-08-27 18:56:01,346 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:56:01,627 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g7g6 c7c6 e7e5 c5d4 b8c6      E: -0.91 -1.22 -0.79 -0.68 -0.92           B: c7c6...
2025-08-27 18:56:03,614 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:56:03,614 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 23) ===
2025-08-27 18:56:03,614 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:56:03,614 | grpo_training | DEBUG | Rewards: min=0.212, max=0.212, mean=0.212, std=0.000
2025-08-27 18:56:03,614 | grpo_training | DEBUG | Reward distribution: {'0.2': 64}
2025-08-27 18:56:03,614 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:56:03,614 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:56:03,614 | grpo_training | DEBUG | Completion: M: h8h7 f1e1 f3g5 h1g1 f3e5      E: 999.97 999.97 999.96 999.98 999.97      B: h1g1...
2025-08-27 18:56:03,614 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:56:03,614 | grpo_training | DEBUG | Completion: M: f3g5 d1e1 h8h7 h1g1 f3e5      E: 14.34 11.64 14.39 15.13 13.8            B: h1g1...
2025-08-27 18:56:03,614 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:56:03,614 | grpo_training | DEBUG | Completion: M: d2c1 f1g2 d2g5 h8h7 h1g1      E: 13.04 11.91 13.38 12.98 12.59           B: d2g5...
2025-08-27 18:56:03,830 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 23 | Time: 21.56s
2025-08-27 18:56:04,019 | grpo_training | INFO | 
Step 23 | Time: 21.56s
  Loss: -0.0687
2025-08-27 18:56:04,019 | grpo_training | INFO |   Loss: -0.0687
  PG Loss: -0.0000
2025-08-27 18:56:04,019 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.3015
2025-08-27 18:56:04,019 | grpo_training | INFO |   KL Divergence: -0.3015
2025-08-27 18:56:04,019 | grpo_training | DEBUG |   KL Forward: -0.3015
2025-08-27 18:56:04,019 | grpo_training | DEBUG |   KL Reverse: 1.2303
2025-08-27 18:56:04,019 | grpo_training | DEBUG |   KL Symmetric: 0.4644
2025-08-27 18:56:04,019 | grpo_training | DEBUG |   Ratio Outliers: 64.062%
2025-08-27 18:56:04,019 | grpo_training | DEBUG |   Gradient Norm: 0.3942
2025-08-27 18:56:04,128 | grpo_training | DEBUG | 
Step 24: Sampled 8 prompts
2025-08-27 18:56:04,129 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:56:04,129 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:56:04,501 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g3g2 g3h3 h7h5 h7h6 b7b6      E: -1.6 -0.61 -1.8 -2.04 -2.05             B: b7b6...
2025-08-27 18:56:07,025 | grpo_training | DEBUG | Processing sample 2/8: A task
2025-08-27 18:56:07,179 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 8/2p5/p1P3P1/1p6/5Q2/7p/PPP1RPkP/2K5 b - - 10 42+0.001+0+0...
2025-08-27 18:56:08,254 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:56:08,551 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b2b4 c1g5 b1d2 c4b3 f1e1 h2h3      E: 0.35 0.18 0.2 0.21 0.15                 B: b2b4...
2025-08-27 18:56:10,574 | grpo_training | DEBUG | Processing sample 4/8: A task
2025-08-27 18:56:10,732 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 3r2k1/8/1pP4Q/4p3/8/1P2BN1B/5P2/2KR3R w - - 1 35+0.001+0+0...
2025-08-27 18:56:11,833 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:56:12,126 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g2g4 g1f3 c1e3 f1e2 e4e5      E: 1.03 1.1 1.13 1.1 1.18                  B: e4e5...
2025-08-27 18:56:14,151 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:56:14,435 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: a6b4 e7b4 f6d5 c8d7 c7c5      E: -0.71 -0.94 -0.61 -0.88 -0.77           B: e7b4...
2025-08-27 18:56:16,441 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:56:16,741 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c3c2 c3e3 f3d4 f1e2 h5g6      E: 7.68 7.7 7.93 7.68 9.07                 B: h5g6...
2025-08-27 18:56:18,834 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:56:19,138 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h7h6 e8e5 f5g6 a7a6 g7g6      E: -0.17 -0.16 -0.14 0.09 -0.12            B: f5g6...
2025-08-27 18:56:21,276 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:56:21,277 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 24) ===
2025-08-27 18:56:21,277 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:56:21,277 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.409, std=0.341
2025-08-27 18:56:21,277 | grpo_training | DEBUG | Reward distribution: {'0.2': 48, '1.0': 16}
2025-08-27 18:56:21,277 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:56:21,277 | grpo_training | DEBUG | 
[Sample 1] Reward: 0.212
2025-08-27 18:56:21,277 | grpo_training | DEBUG | Completion: M: g3g2 g3h3 h7h5 h7h6 b7b6      E: -1.6 -0.61 -1.8 -2.04 -2.05             B: b7b6...
2025-08-27 18:56:21,277 | grpo_training | DEBUG | 
[Sample 2] Reward: 0.212
2025-08-27 18:56:21,277 | grpo_training | DEBUG | Completion: M: g3g2 b7b6 a7a5 a7a6 g3h3      E: -2.21 -2.52 -2.62 -2.64 -1.97           B: a7a6...
2025-08-27 18:56:21,277 | grpo_training | DEBUG | 
[Sample 3] Reward: 0.212
2025-08-27 18:56:21,277 | grpo_training | DEBUG | Completion: M: b7b6 g3h3 g3g1 a7a5 g7h6      E: -1.86 -0.96 -2.29 -1.96 -1.78           B: g3g1...
2025-08-27 18:56:21,467 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 24 | Time: 17.50s
2025-08-27 18:56:21,641 | grpo_training | INFO | 
Step 24 | Time: 17.50s
  Loss: -0.0461
2025-08-27 18:56:21,641 | grpo_training | INFO |   Loss: -0.0461
  PG Loss: -0.0000
2025-08-27 18:56:21,641 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: 0.0445
2025-08-27 18:56:21,641 | grpo_training | INFO |   KL Divergence: 0.0445
2025-08-27 18:56:21,641 | grpo_training | DEBUG |   KL Forward: 0.0445
2025-08-27 18:56:21,641 | grpo_training | DEBUG |   KL Reverse: 0.8355
2025-08-27 18:56:21,641 | grpo_training | DEBUG |   KL Symmetric: 0.4400
2025-08-27 18:56:21,641 | grpo_training | DEBUG |   Ratio Outliers: 48.438%
2025-08-27 18:56:21,641 | grpo_training | DEBUG |   Gradient Norm: 0.2370
2025-08-27 18:56:21,740 | grpo_training | DEBUG | 
Step 25: Sampled 8 prompts
2025-08-27 18:56:21,740 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:56:21,740 | grpo_training | DEBUG | Processing sample 1/8: A task
2025-08-27 18:56:21,950 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: r1b2b2/1p3Q2/3k1P1p/p1nN2p1/2P5/5N2/PP3PPP/R1B1KB1R w KQ - 1 18+0.001+0+0...
2025-08-27 18:56:23,356 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:56:23,652 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d1e1 d1b3 g1h1 b1c2 f3e1 b1a2      E: -0.48 -0.64 -0.79 -0.65 -0.75           B: d1e1...
2025-08-27 18:56:25,640 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:56:25,938 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d1a4 b1c3 b1a3 f1e2 c1e3      E: 3.47 3.52 3.05 2.72 2.87                B: b1c3...
2025-08-27 18:56:28,054 | grpo_training | DEBUG | Processing sample 4/8: A task
2025-08-27 18:56:28,250 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 5r1r/1b1p1pp1/n3pn2/BN2N3/p1p1P3/P6P/1PP1BPP1/R2R2K1 b - - 0 23+0.001+0+0...
2025-08-27 18:56:29,613 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:56:29,913 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h7h5 f5d6 d8f6 d8a5 d8b6      E: -2.64 -3.05 -1.57 -3.03 -2.64           B: f5d6...
2025-08-27 18:56:32,033 | grpo_training | DEBUG | Processing sample 6/8: P task
2025-08-27 18:56:32,412 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: g2h1 g2h2 g2f1 h3g3           E: -9.44 -6.4 -7.87 -8.26                  B: g2h2...
2025-08-27 18:56:35,044 | grpo_training | DEBUG | Processing sample 7/8: A task
2025-08-27 18:56:35,210 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 6nr/3p1p2/1k1b4/p2N2pp/2R5/2PKB3/5PPP/5B1R b - - 1 34+0.001+0+0...
2025-08-27 18:56:36,370 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:56:36,670 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c8d7 e6e5 b6c7 b6d4 b6c6      E: 0.24 0.26 0.18 0.42 0.17                B: b6c7...
2025-08-27 18:56:38,777 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:56:38,778 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 25) ===
2025-08-27 18:56:38,778 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:56:38,778 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.508, std=0.381
2025-08-27 18:56:38,778 | grpo_training | DEBUG | Reward distribution: {'1.0': 24, '0.2': 40}
2025-08-27 18:56:38,778 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:56:38,778 | grpo_training | DEBUG | 
[Sample 1] Reward: 1.000
2025-08-27 18:56:38,778 | grpo_training | DEBUG | Completion: r1b2b2/1p3Q2/3k1P1p/p1nN2p1/2P5/5N2/PP3PPP/R1B1KB1R w KQ - 1 18+0.001+0+0...
2025-08-27 18:56:38,778 | grpo_training | DEBUG | 
[Sample 2] Reward: 1.000
2025-08-27 18:56:38,778 | grpo_training | DEBUG | Completion: r1b2b2/1p3Q2/3k1P1p/p1nN2p1/2P5/5N2/PP3PPP/R1B1KB1R w KQ - 1 18+0.001+0+0...
2025-08-27 18:56:38,778 | grpo_training | DEBUG | 
[Sample 3] Reward: 1.000
2025-08-27 18:56:38,778 | grpo_training | DEBUG | Completion: r1b2b2/1p3Q2/3k1P1p/p1nN2p1/2P5/5N2/PP3PPP/R1B1KB1R w KQ - 1 18+0.001+0+0...
2025-08-27 18:56:38,994 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 25 | Time: 17.43s
2025-08-27 18:56:39,183 | grpo_training | INFO | 
Step 25 | Time: 17.43s
  Loss: -0.0489
2025-08-27 18:56:39,183 | grpo_training | INFO |   Loss: -0.0489
  PG Loss: -0.0000
2025-08-27 18:56:39,183 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: -0.3414
2025-08-27 18:56:39,183 | grpo_training | INFO |   KL Divergence: -0.3414
2025-08-27 18:56:39,183 | grpo_training | DEBUG |   KL Forward: -0.3414
2025-08-27 18:56:39,183 | grpo_training | DEBUG |   KL Reverse: 0.9044
2025-08-27 18:56:39,183 | grpo_training | DEBUG |   KL Symmetric: 0.2815
2025-08-27 18:56:39,183 | grpo_training | DEBUG |   Ratio Outliers: 39.062%
2025-08-27 18:56:39,183 | grpo_training | DEBUG |   Gradient Norm: 0.4216
2025-08-27 18:56:39,290 | grpo_training | DEBUG | 
Step 26: Sampled 8 prompts
2025-08-27 18:56:39,290 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:56:39,290 | grpo_training | DEBUG | Processing sample 1/8: A task
2025-08-27 18:56:39,464 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: Qnbqk2r/5ppp/3bpn2/p7/2PP4/8/PP3PPP/RNBQKBNR w KQk - 1 8+0.001+0+0...
2025-08-27 18:56:40,626 | grpo_training | DEBUG | Processing sample 2/8: P task
2025-08-27 18:56:40,948 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: c2a3 f3f4 d2d1 a4a5 g2g4      E: -0.22 0.17 -0.06 0.02 -0.2              B: f3f4...
2025-08-27 18:56:43,244 | grpo_training | DEBUG | Processing sample 3/8: P task
2025-08-27 18:56:43,605 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: f6g7 f6e5 d4d3 b6c5 h6h5      E: -5.72 -4.76 4.95 6.25 -4.83             B: f6g7...
2025-08-27 18:56:46,144 | grpo_training | DEBUG | Processing sample 4/8: P task
2025-08-27 18:56:46,475 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e8f7                          E: -999.95                                 B: e8f7...
2025-08-27 18:56:48,801 | grpo_training | DEBUG | Processing sample 5/8: P task
2025-08-27 18:56:49,157 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: b8c7 b8b5 b8a7 g8f8 g8f7      E: 4.24 4.28 4.3 4.34 4.22                 B: b8c7...
2025-08-27 18:56:51,644 | grpo_training | DEBUG | Processing sample 6/8: A task
2025-08-27 18:56:51,835 | grpo_training | DEBUG |   K=1, Reward=1.000, Completion: 4r1k1/5p2/p7/1p1P3p/2p1P1P1/P1N5/1P1QB1P1/R3N1KR b - - 0 28+0.001+0+0...
2025-08-27 18:56:53,174 | grpo_training | DEBUG | Processing sample 7/8: P task
2025-08-27 18:56:53,457 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: d5h5 e8c8 a8d8 g4h5 d5f5      E: 0.17 0.21 0.07 0.16 0.12                B: a8d8...
2025-08-27 18:56:55,450 | grpo_training | DEBUG | Processing sample 8/8: P task
2025-08-27 18:56:55,765 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: h5g6 h5e5 d4g4 d4e4 h5g4      E: 1.6 1.96 0.84 0.89 2.52                 B: d4g4...
2025-08-27 18:56:58,004 | grpo_training | DEBUG | 
Advantages: mean=0.000, std=0.000
2025-08-27 18:56:58,004 | grpo_training | DEBUG | 
=== ROLLOUT DETAILS (Step 26) ===
2025-08-27 18:56:58,004 | grpo_training | DEBUG | Number of samples: 64
2025-08-27 18:56:58,004 | grpo_training | DEBUG | Rewards: min=0.212, max=1.000, mean=0.409, std=0.341
2025-08-27 18:56:58,004 | grpo_training | DEBUG | Reward distribution: {'1.0': 16, '0.2': 48}
2025-08-27 18:56:58,004 | grpo_training | DEBUG | 
Sample completions:
2025-08-27 18:56:58,004 | grpo_training | DEBUG | 
[Sample 1] Reward: 1.000
2025-08-27 18:56:58,004 | grpo_training | DEBUG | Completion: Qnbqk2r/5ppp/3bpn2/p7/2PP4/8/PP3PPP/RNBQKBNR w KQk - 1 8+0.001+0+0...
2025-08-27 18:56:58,004 | grpo_training | DEBUG | 
[Sample 2] Reward: 1.000
2025-08-27 18:56:58,004 | grpo_training | DEBUG | Completion: Qnbqk2r/5ppp/3bpn2/p7/2PP4/8/PP3PPP/RNBQKBNR w KQk - 1 8+0.001+0+0...
2025-08-27 18:56:58,004 | grpo_training | DEBUG | 
[Sample 3] Reward: 1.000
2025-08-27 18:56:58,004 | grpo_training | DEBUG | Completion: Qnbqk2r/5ppp/3bpn2/p7/2PP4/8/PP3PPP/RNBQKBNR w KQk - 1 8+0.001+0+0...
2025-08-27 18:56:58,219 | grpo_training | DEBUG | Adaptive KL coefficient: 0.001000

Step 26 | Time: 19.11s
2025-08-27 18:56:58,405 | grpo_training | INFO | 
Step 26 | Time: 19.11s
  Loss: -0.0490
2025-08-27 18:56:58,405 | grpo_training | INFO |   Loss: -0.0490
  PG Loss: -0.0000
2025-08-27 18:56:58,405 | grpo_training | INFO |   PG Loss: -0.0000
  KL Divergence: 0.1329
2025-08-27 18:56:58,405 | grpo_training | INFO |   KL Divergence: 0.1329
2025-08-27 18:56:58,405 | grpo_training | DEBUG |   KL Forward: 0.1329
2025-08-27 18:56:58,405 | grpo_training | DEBUG |   KL Reverse: 0.9396
2025-08-27 18:56:58,405 | grpo_training | DEBUG |   KL Symmetric: 0.5362
2025-08-27 18:56:58,405 | grpo_training | DEBUG |   Ratio Outliers: 50.000%
2025-08-27 18:56:58,405 | grpo_training | DEBUG |   Gradient Norm: 0.2546
2025-08-27 18:56:58,511 | grpo_training | DEBUG | 
Step 27: Sampled 8 prompts
2025-08-27 18:56:58,511 | reward_scorer | INFO | RewardScorer initialized - shaping: graduated, range: [-0.3, 1.0], continuous: ['fen_similarity', 'evaluations']
2025-08-27 18:56:58,511 | grpo_training | DEBUG | Processing sample 1/8: P task
2025-08-27 18:56:58,854 | grpo_training | DEBUG |   K=1, Reward=0.212, Completion: M: e8e2 b2b3 a2a4 a2b3 a2a6 b2b6      E: 4.97 4.58 5.07 4.98 4.42                B: b2b6...
